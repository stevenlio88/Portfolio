[{"content":"\rI specialize in developing tools and implementing advanced Machine Learning solutions to tackle challenges within Data Science and Data-Driven Analytics.\nLearn More\r","date":null,"permalink":"/","section":"","summary":"I specialize in developing tools and implementing advanced Machine Learning solutions to tackle challenges within Data Science and Data-Driven Analytics.","title":""},{"content":"Interesting projects I\u0026rsquo;ve done in the past.\n","date":null,"permalink":"/projects/","section":"Projects","summary":"Interesting projects I\u0026rsquo;ve done in the past.","title":"Projects"},{"content":"Random things I wrote.\n","date":null,"permalink":"/blog/","section":"Blog","summary":"Random things I wrote.","title":"Blog"},{"content":"","date":null,"permalink":"/tags/image-processing/","section":"Tags","summary":"","title":"Image Processing"},{"content":"\rIntroduction #This blog quickly demonstrates the Image Quantization (or Color Quantization) image processing technique. The process involves reducing the number of distinct colors used in an image using a three-dimensional clustering algorithm. Before diving into the why and how, let\u0026rsquo;s briefly discuss what a digital image is.\nWhat is a digital Image #A digital image is typically composed of Width x Height pixels. In color images using the 24-bit RGB (Red, Green Blue) color format each pixel is represented by three integer values ranging from 0 to 255, corresponding to the intensity of each color channel. This format allow for a total of 28 possible values per channel, resulting in 2563 (16,777,216) potential colors.\nPicture of a colorful parrot and the RGB colors:\rWhy? #So why quantization? One of the biggest application of image quantization is to optimizes memory usage by reducing the complexity of color data while maintaining sufficient visual information fo many practical purposes. This reduction in color information not only conserves memory but also enhances the efficiency of subsequent compression techniques.\nAnd it is fun.\nHow? #Image quantization involves clustering similar pixel colors into groups and assigning pixels to representative cluster colors (such as average, median, min, max etc.). Since we are doing clustering, we can leverage one of the well known unsupervised learning clustering algorithm, the K-Means (note that the algorithm becomes computationally intensive with large images).\nThe goal is to define a set number of color clusters (K) from the image, where each pixel in the original image is then assigned the mean color value of the cluster it belongs to. This process ensures that similar pixels are reassigned to similar color values.\nClick me to see the K-Mean implementation in Python:\rimport cv2 import numpy as np import matplotlib.pyplot as plt def find_palette(img, n_colors=10): \u0026#34;\u0026#34;\u0026#34; Returns the n colors palettes from image Args: img (numpy.ndarray): Input image. n_colors (int): Number of color palettes to return. Returns: labels (numpy.ndarray): Array contains the cluster index for each pixel palette (umpy.ndarray): Array of RGB `n_colors` values of the color clusters \u0026#34;\u0026#34;\u0026#34; pixels = img.reshape(-1, 3) criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, 0.1) ret, labels, palette = cv2.kmeans( np.float32(pixels), n_colors, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS ) return labels, palette # Input image img = cv2.imread(r\u0026#34;parrot.jpg\u0026#34;) #quantize image using 14 color clusters labels, palette = find_palette(img, n_colors=14) #reassign image pixel using cluster average img_quant = palette[labels.flatten()].reshape(img.shape).astype(np.uint8) #display image plt.imshow(img_quant[:,:,::-1]) plt.axis(\u0026#39;off\u0026#39;); Results #\rAbove are the results using clusters of sizes 1, 5, 10, 20 and 40. With a cluster size of 1, the result is essentially the average pixel color value of the entire image. As the number of clusters increases, it looks more like the original image already.\nWith smaller clusters, the image appears \u0026ldquo;patchy\u0026rdquo; and exhibits a gradient effect between the color clusters. This occurs because fewer colors representing the image, resulting in loss of color transitions and fine details.\nHere are more examples with increasing the number of clusters:\nRainbows (up to 40 clusters):\nStarry Night (up to 25 clusters):\nGreen Field (up to 25 clusters):\n","date":"05 Jul, 2024","permalink":"/blog/image_quantization/","section":"Blog","summary":"Exploring image quantization for image processing","title":"Image Quantization"},{"content":"","date":null,"permalink":"/tags/opencv/","section":"Tags","summary":"","title":"OpenCV"},{"content":"","date":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/unsupervised-learning/","section":"Tags","summary":"","title":"Unsupervised Learning"},{"content":"","date":null,"permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"Computer Vision"},{"content":"","date":null,"permalink":"/tags/data-science/","section":"Tags","summary":"","title":"Data Science"},{"content":"","date":null,"permalink":"/tags/javascript/","section":"Tags","summary":"","title":"Javascript"},{"content":"","date":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning"},{"content":"","date":null,"permalink":"/tags/onnx/","section":"Tags","summary":"","title":"ONNX"},{"content":"","date":null,"permalink":"/tags/tensorflow/","section":"Tags","summary":"","title":"Tensorflow"},{"content":"","date":null,"permalink":"/tags/web-application/","section":"Tags","summary":"","title":"Web Application"},{"content":"\rMountain Goats at Banff, AB, Canada in Aug, 2022 Â© Steven Lio\rUpload an image and I will tell you it is Upgoat or Downgoat!\rGo Predict!\rIntroduction #Inspired by the challenged posed on StackExchange titled Upgoat or Downgoat?. The challenge originally asked participants to create a program to determine if an image of a goat is upside down, or not.\nThis project is not aimed to solve the challenge under its strict conditions, my focus on this project is to deploy a pre-trained Neural Network and make predictions directly in the browser. This project will involve training a Neural Network to solve the Upgoat or Downgoat? problem using Tensorflow, then convert to ONNX model format and implement JavaScript to handle the uploaded image processing and make prediction using the Neural Network.\nProcess #1. Image Data #I\u0026rsquo;ve started with 3,262 images that contains goats from various sources1. Then I\u0026rsquo;ve compile all the images and remove duplicates images. Some of these images contains more than one goat or it is a partial of a goat. To create the training datasets, I\u0026rsquo;ve first used the famous YOLOv32 to find all goats from the images and cropped out the regions that containing the goats, then output image of each goat found in the images resulting 3,473 images containing at most one/partial of a goat.\nTo build the final training datasets, I\u0026rsquo;ve randomly split the images into 80% for training, 15% for validation and 5% for testing. Then for each batch randomly rotate half of them upside down to create the images of \u0026ldquo;downgoats\u0026rdquo;.\nClick me to see the YOLO implementation in Python:\rimport cv2 import numpy as np import matplotlib.pyplot as plt # Load YOLO yolo_version = \u0026#34;yolov3\u0026#34; net = cv2.dnn.readNet(f\u0026#34;model/{yolo_version}.weights\u0026#34;, f\u0026#34;model/{yolo_version}.cfg\u0026#34;) layer_names = net.getLayerNames() output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()] # Load class names with open(\u0026#34;model/coco.names\u0026#34;, \u0026#34;r\u0026#34;) as f: classes = [line.strip() for line in f.readlines()] def YOLO(yolo, img): \u0026#34;\u0026#34;\u0026#34; Use YOLO to find all identified objects from image. Args: yolo (cv2.dnn.Net): YOLO model object. img (np.ndarray): Image array object. Outpus: class_ids (list): COCO class ids for the objects label confidences (list): Confidences levels of the detected object boxes (list): Bounding box for the detected objects indices (list): Indices of the bounding boxes kept after NMS. \u0026#34;\u0026#34;\u0026#34; # Prepare the image for YOLO blob = cv2.dnn.blobFromImage( img, scalefactor=0.00392, size=(416, 416), mean=(0, 0, 0), swapRB=True, crop=False, ) yolo.setInput(blob) outs = yolo.forward(output_layers) # Initialize parameters class_ids = [] confidences = [] boxes = [] # Process each detection for out in outs: for detection in out: scores = detection[5:] class_id = np.argmax(scores) confidence = scores[class_id] if confidence \u0026gt; 0.5: # Object detected center_x = int(detection[0] * img.shape[1]) center_y = int(detection[1] * img.shape[0]) w = int(detection[2] * img.shape[1]) h = int(detection[3] * img.shape[0]) x = int(center_x - w / 2) y = int(center_y - h / 2) boxes.append([x, y, w, h]) confidences.append(float(confidence)) class_ids.append(class_id) # Apply non-max suppression to remove overlapping boxes indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4) return class_ids, confidences, boxes, indices def find_objects( img, classes_of_interest, class_ids, confidences, boxes, indices, swapRB ): \u0026#34;\u0026#34;\u0026#34; Find objects from image given by the YOLO outputs: Args: img (np.ndarray): Image array object classes_of_interest (list): List of Class Id to extract objects base on the COCO classes class_ids (list): List of class id of the objects identified by YOLO confidences (list): Confidences levels of the detected object boxes (list): Bounding box for the detected objects indices (list): Indices of the bounding boxes kept after NMS. swapRB (bool): For displaying purposes, flip the Red and Blue channel in the image Outputs: img_out (np.ndarray): Ouput image with object box objects (dict): Dictionary of the selected objects and its bounding box \u0026#34;\u0026#34;\u0026#34; img_out = img.copy() if swapRB: img_out = cv2.cvtColor(img_out, cv2.COLOR_BGR2RGB) objects = {} if isinstance(indices, np.ndarray) and len(indices.shape) == 2: indices = indices.flatten() for i in indices: class_id = class_ids[i] class_name = classes[class_id] if class_name in classes_of_interest: x, y, w, h = boxes[i] cv2.rectangle(img_out, (x, y), (x + w, y + h), (0, 255, 0), 2) label = f\u0026#34;{class_name}: {confidences[i]:.2f}\u0026#34; cv2.putText( img_out, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, ) objects[class_name] = {\u0026#34;id\u0026#34;: class_id, \u0026#34;box\u0026#34;: (x, y, w, h)} return img_out, objects # Found goats img = cv2.imread(r\u0026#34;goats.jpg\u0026#34;) class_ids, confidences, boxes, indices = YOLO(net, img) classes_of_interest = classes[14:24] # animals img_out, objects = find_objects( img, classes_of_interest, class_ids, confidences, boxes, indices, True ) plt.imshow(img_out) plt.axis(\u0026#34;off\u0026#34;) Note: YOLOv3 is not trained to identify goat specifcally, however it is good at identify what is goat-like animal (e.g. cow, horse, dog etc) vs. what is not.\n2. Train Neural Network #I\u0026rsquo;ve decided to build the Neural Network using Tensorflow3 to see how different it is to PyTorch4. Instead of training a new Neural Network, I\u0026rsquo;ve transfer learning from the MobileNetV2 to extract images features and create a custom layer in the final model then train it to identify upgoats vs downgoats.\nThe resulting model is able to input RGB images of size 128x128 and produce a probability of it being an upgoat (p=1) or downgoat(p=0). The trained model performed exceptional on validation images where after tuning it achieved ~99.9% in all metrics and ~0% loss. This does not mean the model has the ability to determine upgoat vs. downgoat 99.9% all the time. But it is very good with the images available and to decide if an goat-like object is right-side-up or up-side-down, and probably still good at determining yours.\nHere are the details of the training process: Notebook\r3. Convert to ONNX #The final model object (.h5) is converted to an ONNX object5. It is an open format built to represent machine learning models available in both Python and JavaScript.\nClick me to see the ONNX conversion in Python:\rimport tensorflow as tf import tf2onnx import onnx # Load the Keras model loaded_model = tf.keras.models.load_model(r\u0026#34;model/goat.h5\u0026#34;) # Define input layers format input_signature = [tf.TensorSpec([None, 128, 128, 3], tf.float32, name=\u0026#34;input\u0026#34;)] onnx_model, _ = tf2onnx.convert.from_keras(loaded_model, input_signature, opset=13) onnx.save(onnx_model, r\u0026#34;model/goat.onnx\u0026#34;) Deployment #The implementation of incorporating an ONNX model in JavaScript is straight forward using the ONNX Runtime library6.\nClick me to see the model implementation in JavaScript:\rasync function loadModel(modelPath) { try { // Load the ONNX model const model = await ort.InferenceSession.create(modelPath); return model; } catch (error) { console.error(\u0026#39;Error loading model:\u0026#39;, error); throw error; } } const model_path = ... # Path to the model const model = await loadModel(model_path); const input = ... # Define the necessary input for your model const output = await model.run(input); Future Improvements #Add YOLO to the app, find if image do contains a goat like subject.\nWhat I\u0026rsquo;ve learned #I\u0026rsquo;ve successfully implemented a Neural Network with ONNX as well as training ONNX. Also incorporating YOLO or other similar image classification models for image processing greatly enhanced the ability to create robust training images for computer vision tasks.\nReferences #1 Goat images: Dataset Ninja, Mendeley Data, images.cv 2YOLOv3 3Tensorflow 4PyTorch 5Open Neural Network Exchange (ONNX) 6ONNX Runtime\n","date":"01 Jul, 2024","permalink":"/projects/goat/","section":"Projects","summary":"Image classification in browser","title":"Web Application: Upgoat or Downgoat?"},{"content":"","date":null,"permalink":"/tags/yolo/","section":"Tags","summary":"","title":"YOLO"},{"content":"","date":null,"permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"Deep Learning"},{"content":"\rPicture taken at Honolulu, HI, USA in Apr, 2024 Â© Steven Lio\rIntroduction #The primary objective of this project is to create image mask accurately to hightlight the areas of baby corals growth from images of coral plugs. This will aid in the research of understanding growth patterns of baby corals, collect data on various parameters such as the size, dry weight, and growth / mortality rates in baby corals to evaluate the effectiveness of some treatments designed to enhance coral growth.\nCoral protection is essential for maintaining biodiversity in the marine ecosystems and significantly impacts global food production.\nHaving visited Honolulu, HI many times, I personally witnessed the devastating impact of ocean warming on coral bleaching and the consequent decline in marine life diversity.\nBackground #Image Segmentation1 is a well-studied topic in computer visions research long before the advancement in modern-days GPUs technology (notably Nvidia) that have enabled the development and application of more complex algorithms, such as Convolution Neural Networks (CNNs)2 for computer vision tasks.\nHowever, developing and training a CNN is very costly, and more importantly, requires a significant amount of representative training data to achieve accurate and reliable results.\nFor this project, I used an image set of coral plugs (total 130 images), consisting of top-down views of various individual circular coral plugs. Due to confidentiality, I cannot disclose the data source.\nHowever, I can show you an example of the images I used:\nImage example\rCoral baby (Middle)\rPreprocessing Images #The coral plug images were captured from a systems that was calibrated to centered the coral plugs in the image. However, there are variances where some are not as centered. The images captured were also from the same distance, each coral plug will appears to be nearly identical in size.\nThe images were first preprocessed by centering them and cropping the key area (i.e. coral plug) to ensure when developing an algorithm, the machine learning algorithm focuses only on the coral plug, minimizing distractions.\nTo achieve this, I used the Hough Circle Transform3 algorithm which is specifically designed for detecting \u0026ldquo;circles\u0026rdquo; in an image. This is ideal for my images as it helps locate the coral plug, find its center and crop out the excess areas.\nClick me to see the implementation in Python:\r# OpenCV library import cv2 # Path for image and output folder img_path = r\u0026#34;coral.jpg\u0026#34; out_path = r\u0026#34;out_folder\u0026#34; # Read image and resize to 512 x 512 img = cv2.imread(img_path) img = cv2.resize(img, (512, 512)) # Create a copy of the original image and a gray scale version of the image output = img.copy() gray = cv2.cvtColor(output, cv2.COLOR_BGR2GRAY) gray = cv2.medianBlur(gray, 5) # Padding the correct circle padding = 8 # Find Hough Circles circles = cv2.HoughCircles( gray, cv2.HOUGH_GRADIENT, dp=1.2, minDist=120, param1=100, param2=30, minRadius=120, maxRadius=180, ) # Draw Hough Circles try: circles = np.round(circles[0, :]).astype(\u0026#34;int\u0026#34;) for (x, y, r) in circles: if 200 \u0026lt; x \u0026lt; 250 and 200 \u0026lt; y \u0026lt; 250: cv2.circle(output, (x, y), r + padding, (0, 255, 0), 4) print(x, y, r) else: cv2.circle(output, (x, y), r, (0, 0, 255), 2) except: print(\u0026#34;No circles is found! Check Hough Circle parameters.\u0026#34;) # Save the final output to folder cv2.imwrite(\u0026#34;{0}\\{1}\u0026#34;.format(out_path, \u0026#34;houghcircles.jpg\u0026#34;), output) The output after applying Hough Circle Transform:\nIn the output image, correctly detected circles are marked in green, while incorrect ones are in red. Knowing the size of the image and the coral plug\u0026rsquo;s scale allowed me to constrain the Hough Circle algorithm to a specific radius range and focus on the center of the image. After identifying the key area, I cropped out this region and create a black mask to cover the remaining areas outside of the coral plug.\nClick me to see the final implementation in Python:\rdef find_plug(img, padding=5, create_mask=True, out_size=(512, 512)): \u0026#34;\u0026#34;\u0026#34; Find Coral Plug in image using Hough Circle. Args: img (numpy.ndarray): Input image containing coral plug. padding (int): Padding for the correct circle. create_mask (bool): Create a mask as the output. out_size (tuple): Output image size. Returns: int: The product of a and b. \u0026#34;\u0026#34;\u0026#34; # Gray scaling image gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) gray = cv2.medianBlur(gray, 5) # Define Hough Circles with hyper-parameters circles = cv2.HoughCircles( gray, cv2.HOUGH_GRADIENT, dp=1.2, minDist=120, param1=100, param2=30, minRadius=120, maxRadius=180, ) # Find the correct circle if circles is not None: circles = np.round(circles[0, :]).astype(\u0026#34;int\u0026#34;) for (x, y, r) in circles: if 200 \u0026lt; x \u0026lt; 250 and 200 \u0026lt; y \u0026lt; 250: r += padding break else: x, y, r = img.shape[0] // 2, img.shape[1] // 2, 80 break # Create the square cropping area based on the found circle cropped_image = img[y - r : y + r, x - r : x + r] # Mask out the areas outside of the circle if create_mask: mask = np.zeros_like(cropped_image) center = cropped_image.shape[0] // 2 cv2.circle(mask, (center, center), r + padding, (255, 255, 255), thickness=-1) cropped_image = cv2.bitwise_and(cropped_image, mask) cropped_image_resized = cv2.resize(cropped_image, out_size) return cropped_image_resized Here is an example of the complete preprocessing applied to each image:\nNaive Approach #Before implementing a more sophisticated model/algorithm, I want to explore a simple and naive method to see how far it can takes me. This is usualy a rule of thumb for all the projects I\u0026rsquo;ve worked on.\nThe naive method I used is called Thresholding4. It is commonly used in digital image processing.\nDespite its simplicity, this method can be very effective for certain types of images without needing any advanced machine learning, in example like images captured in other spectrums (e.g. IR, UV) or images captured using a fluorescence cameras (\\$\\$\\$) which is very effective for highlighting organic materials from its surrounding.\nExample of Fluorescent Imaging of cancer cells5\rHowever, I will be working with regular RGB colored images of coral plugs, also using images captured in other light spectrum may not be as effective on coral plugs. Since there are other creatures like algaes, baby sea anenemon also growing on the coral plug and those may easily being falsly identified.\nHere is the naive approach:\nFind color pixel value range of the coral babies using k-mean on the pixel values to identify the dominant color palettes in RGB of the corals this process is also known as Quantization6. Before Quantization\rAfter Quantization\rHere I found 10 dominant colors for the coral babies.\nFine tuning Thresholding on each of the dominant color clusters range identified in k-mean. The image below shows how the selected area widens as the value range for each color cluster increases. Blindly expanding these ranges can falsly highlight areas that contains no coral babies:\nThis concludes the naive approach, it requires lots of fine tuning and very difficult to generalized especially when dealing with real-world data.\nActually\u0026hellip;Isn\u0026rsquo;t green screen exactly this? Instead of creating a mask, we just replace the green pixels with pixels from another images? Click me to see the naive approach in Python:\rimport cv2 import numpy as np import glob import matplotlib.pyplot as plt def imshow_pair(img1, img2): \u0026#34;\u0026#34;\u0026#34; Display pair of images with same height side by side Args: img1 (numpy.ndarray): left image img2 (numpy.ndarray): right image returns \u0026#34;\u0026#34;\u0026#34; fig, ax = plt.subplots(1, 2) ax[0].imshow(img1) ax[0].axis(\u0026#34;off\u0026#34;) ax[1].imshow(img2) ax[1].axis(\u0026#34;off\u0026#34;) def concat_vertical(img_list, interpolation=cv2.INTER_CUBIC): \u0026#34;\u0026#34;\u0026#34; Concat images of different sizes vertically. Images are resize such that it will match the mininum image width in the image list Args: img_list (list): List of image to concat. interpolation (int): OpenCV interpolation method for resizing Returns: int: The product of a and b. \u0026#34;\u0026#34;\u0026#34; min_width = min(img.shape[1] for img in img_list) img_list_resize = [ cv2.resize( img, (min_width, int(img.shape[0] * min_width / img.shape[1])), interpolation=interpolation, ) for img in img_list ] return cv2.vconcat(img_list_resize) def find_palette(img, n_colors=10): \u0026#34;\u0026#34;\u0026#34; Returns the n colors palettes from image Args: img (numpy.ndarray): Input image. n_colors (int): Number of color palettes to return. Returns: labels (numpy.ndarray): Array contains the cluster index for each pixel palette (umpy.ndarray): Array of RGB `n_colors` values of the color clusters \u0026#34;\u0026#34;\u0026#34; pixels = img.reshape(-1, 3) criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, 0.1) ret, labels, palette = cv2.kmeans( np.float32(pixels), n_colors, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS ) #_, counts = np.unique(labels, return_counts=True) #palette = palette[np.argsort(counts)[::-1]] return labels, palette def create_mask(img, palette, pixel_range_window=5): \u0026#34;\u0026#34;\u0026#34; Create mask from image based on palette information Args: img (numpy.ndarray): Input image. palette (numpy.ndarray): Color palette values. pixel_range_window (int): width of the color range for each palette value Returns: coral_withMask (numpy.ndarray): The original coral image \u0026#34;\u0026#34;\u0026#34; new_img_blur = cv2.GaussianBlur(img, (15, 15), 0) new_img_flat = cv2.cvtColor(new_img_blur, cv2.COLOR_BGR2HSV).reshape((-1, 3)) mask = np.uint8(np.zeros(new_img_blur.shape[:2])) for i in range(len(palette)): lower = palette[i] - pixel_range_window upper = palette[i] + pixel_range_window mask = cv2.bitwise_or(mask, cv2.inRange(new_img_blur, lower, upper)) no_coral = cv2.bitwise_and(img, img, mask=cv2.bitwise_not(mask)) coral = cv2.bitwise_and(img, img, mask=mask) coral_withMask = cv2.addWeighted(no_coral, 0.3, coral, 1.5, 0.0) return coral_withMask, mask # import image references of manually selected coral regions references = glob.glob(r\u0026#34;coral_references\\*.jpg\u0026#34;) # stitch all references into one for analysis stitched_references = cv2.resize( cv2.imread(references[0]), (256, 256), interpolation=cv2.INTER_CUBIC ) for i in references[1:]: stitched_references = concat_vertical( [ stitched_references, cv2.resize(cv2.imread(i), (256, 256), interpolation=cv2.INTER_CUBIC), ] ) # Using k-means to find palettes: labels, palette = find_palette(stitched_references, n_colors=10) # Create mask on image: img_path = r\u0026#34;img.jpg\u0026#34; img = cv2.imread(img_path) coral_withMask, coral_mask = create_mask(img, palette, pixel_range_window=12) imshow_pair( cv2.cvtColor(coral_withMask, cv2.COLOR_BGR2RGB), cv2.cvtColor(coral_mask, cv2.COLOR_BGR2RGB), ) Convolution Neural Network #Convolution Neural Network is popular in solving computer vision tasks, I chose the U-Net7 architecture from the Convolution Neural Networks2 architures family. It is well known for its proven sucess in solving image segmentation problems and its ablility to trained efficiently to produce remarkable results for image segmentation.\nSince I didn\u0026rsquo;t have a existing mask for each coral plug image, I manually created them for each image. These masks highlight the areas where the coral babies are located on the coral plug. The objective of the Neural Network is to use these image-mask pairs to train and accurately generate masks for new images of coral plugs.\nThe U-Net training process follows the standard Neural Network training procedures: spliting images into training, validation and test sets to assess model\u0026rsquo;s performance, defining loss function, optimizer and approporiate evaluation metrics to monitor model outputs during training, applied early stopping to control overfitting, and evaluating the best model on unseen images.\nTransfer Learning #I\u0026rsquo;ve opted to use the U-Net architecture with a ResNet8 encoder backbone and begin with pre-trained weights from the ImageNet9 project. This approach allows me to leverage transfer learning from a more sophisticated model that has been exposed to extensive image data, rather than developing an entirely new architecture from scratch. Additionally, given that I only have an Nvidia RTX 2060 with 6GB GPU, training a larger model from the ground up isn\u0026rsquo;t feasible.\nModel Configs # Image size: 256x256 Batch size: 32 Epoch10: 500 (With Early Stopping11)) Loss Criteria12: Jaccard Loss13 ~ IoU14 Activation Function15: Sigmoid Optimizer16: Adam17 Learning Rate18: 0.001 Results #The training process early stopped after only 83 epochs to avoid overfitting.\nIn terms of metrics, the model demostrated great success in learning and generating accurate masks for the training data. However, its performance on unseen images (validation) wasn\u0026rsquo;t optimal, although still quite satisfactory.\nKey takeaways: the best-performing model achieved a 69.9% Intersection over Union (IoU)14 score and an 78.9% recall rate.\nUpon analyzing the performance metrics, there\u0026rsquo;s a noticeable divergence between the validation images (unseen during training) and the training images. This indicates that the model is susceptible to overfitting, especially given the limited amount of images available which results in a small validation set - typically associated with higher variance. Also, the validation set may include specific cases that were not represented in the training data.\nSome prediction results on unseen images:\nPredict on test image - IoU (74.1%); Left to right: Coral Plug, True Mask, Predicted Mask\rPredict on image without preprocessing; Coral Plug vs Predicted Mask\rPredict on Lenna\rWhat I\u0026rsquo;ve learned #I\u0026rsquo;ve gained significant insights and hands-on experience developing a Neural Network for image segmentation on real-world images. There are many other things to explore for improving the model such as fine tuning trainin parameters, trying different CNN architecture, finding larger datasets and refining the true masks so the model will learn precisely what a baby coral is.\nReferences #Here is the Jupyter notebook on the model building process.\nNotebook\r1\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rImage Segmentation 2\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rConvolutional Neural Network 3Hough Circle Transform 4\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rThresholding 5Credit: Nicola Ferrari @iStock 6\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rQuantization 7\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rU-Net 8\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rResnet 9\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rImageNet 10\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rEpoch 11\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rEarly Stopping 12\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rLoss Criteria 13\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rJaccard Loss 14\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rIoU 15\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rActivation Function 16\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rOptimizer 17\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rAdam 18\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rLearning Rate\n","date":"06 Jan, 2023","permalink":"/projects/coral_baby/","section":"Projects","summary":"Process of creating robust image segmentation of coral babies","title":"Image Segmentation of Coral Baby"},{"content":"","date":null,"permalink":"/tags/neural-network/","section":"Tags","summary":"","title":"Neural Network"},{"content":"","date":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"PyTorch"},{"content":"","date":null,"permalink":"/tags/transfer-learning/","section":"Tags","summary":"","title":"Transfer Learning"},{"content":"","date":null,"permalink":"/tags/classification/","section":"Tags","summary":"","title":"Classification"},{"content":"","date":null,"permalink":"/tags/feature-engineering/","section":"Tags","summary":"","title":"Feature Engineering"},{"content":"\rPhoto of a Photo Â© Steven Lio\rIntroduction #The goal of this project was to develope a classification machine learning model as a proof-of-concept capable of reliably detecting wheather an image is an original photograph or a scanned reproduction of another picture for Trusting Pixel Inc. (Vancouver, Canada). This project served as my Capstone project during my Master of Data Science program at the University of British Columbia where I collaborated with three other students.\nDisclaimer: Due to NDA, I am not able to disclose details of the model but I will document the process of discovery and challenges we faced during the project.\nBackground #Image authenticity recognition is crucial for upholding trust and credibility in digital content, preventing misinformation, and protecting intellectual property. Advancements in deep learning models like DALL-E which is capable of generating realistic images which prompt more reseaches focus into image authenitication. But I believe most of these researches maybe hidden from the public because revealing the principle behind any security system will immediately compromise its effectiveness.\nFor most people, likely the \u0026ldquo;photo of a photo\u0026rdquo; problem isn\u0026rsquo;t significant. However, for companies in the fashion and entertainment industries, they rely heavily on reviewing unaltered professional headshots or body shots of the models, actors, and actresses they are considering for hire.\nThis project piqued my interest because I have been experimenting with different computer vision techniques even before getting my master\u0026rsquo;s degree and have always wanted to gain more experiences in developing Neural Networks.\nData Preprocessing #Trusting Pixel Inc. provided the project with a collection of geninue digital images as well as their recaptured version on the printed photo. The first challenge we faced was the lack of linkage between the geninue images and their recaptured counterparts. This is important for developing a model that extract clues and evidence unique to recaptured images of photos and not memorizing the subjects in the training images.\nI developed a process in Python for automatically pairing the geninue image with its recaptured counterpart and return the pair in the same orientation.\nIn order to find the similar pair of images, I\u0026rsquo;ve first calculated a similarity score (0 to 1) between the image and all the other images in the set. Then the image in the set with the highest score will get paired up. Then repeat until all images has found at least a pair. I\u0026rsquo;ve calculated a similarity score for each possible pair of images using the SIFT1 algorithm.\nIt worked amazingly well even when I am trying to pair a geninue images with a picture of its printed photo.\nClick here to see a implementation of SIFT Similarity calculation in Python:\rimport cv2 import matplotlib.pyplot as plt def sift_similarity(img1, img2): \u0026#34;\u0026#34;\u0026#34; Calculate similarity scores based on SIFT descriptors Args: img1 (numpy.ndarray): Input image 1 img2 (numpy.ndarray): Input image 2 Returns: float: similarity scores between image 1 and image 2 \u0026#34;\u0026#34;\u0026#34; sift = cv2.SIFT_create() kp_1, desc_1 = sift.detectAndCompute(img1, None) kp_2, desc_2 = sift.detectAndCompute(img2, None) index_params = dict(algorithm=0, trees=5) search_params = dict() flann = cv2.FlannBasedMatcher(index_params, search_params) matches = flann.knnMatch(desc_1, desc_2, k=2) good_points = [] ratio = 0.6 for m, n in matches: if m.distance \u0026lt; ratio*n.distance: good_points.append(m) #calculate similarity score: number_keypoints = 0 if len(kp_1) \u0026lt;= len(kp_2): number_keypoints = len(kp_1) else: number_keypoints = len(kp_2) try: match_rate = len(good_points) / number_keypoints except: match_rate = 0 return match_rate # Test on images img1 = cv2.imread(\u0026#34;img1.jpeg\u0026#34;) img2 = cv2.imread(\u0026#34;img2.jpeg\u0026#34;) img3 = cv2.imread(\u0026#34;img3.jpeg\u0026#34;) # Display images fig, axes = plt.subplots(1, 3, figsize=(15,5)) axes[0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)) axes[0].set_title(\u0026#39;Image 1\u0026#39;) axes[0].axis(\u0026#39;off\u0026#39;) axes[1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)) axes[1].set_title(\u0026#39;Image 2\u0026#39;) axes[1].axis(\u0026#39;off\u0026#39;) axes[2].imshow(cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)) axes[2].set_title(\u0026#39;Image 3\u0026#39;) axes[2].axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() # Print scores to console print(\u0026#34;SIFT Similarity between \u0026#39;Image 1\u0026#39; and \u0026#39;Image 1\u0026#39; is {0:.2f}.\u0026#34;.format(sift_similarity(img1, img1))) print(\u0026#34;SIFT Similarity between \u0026#39;Image 1\u0026#39; and \u0026#39;Image 2\u0026#39; is {0:.2f}.\u0026#34;.format(sift_similarity(img1, img2))) print(\u0026#34;SIFT Similarity between \u0026#39;Image 2\u0026#39; and \u0026#39;Image 3\u0026#39; is {0:.2f}.\u0026#34;.format(sift_similarity(img2, img3))) Image 1\rImage 2\rImage 3\rThe output from the Python script:\nSIFT Similarity between \u0026#39;Image 1\u0026#39; and \u0026#39;Image 1\u0026#39; is 1.00. SIFT Similarity between \u0026#39;Image 1\u0026#39; and \u0026#39;Image 2\u0026#39; is 0.67. SIFT Similarity between \u0026#39;Image 2\u0026#39; and \u0026#39;Image 3\u0026#39; is 0.26. After having pairing the images, they were moved to a new folder and renamed to keep track of their pairing, making them ready for the next step.\nModeling #Although Convolution Neural Networks (CNNs) are popular for solving computer vision tasks with images data, it wasn\u0026rsquo;t entirely feasible at the time due to our small dataset and limited computing power, making training a large Neural Network unrealistic.\nThat led us to using traditional classification models. However, these models require a feature matrix as input, and feeding the full-sized images would result in an enormous matrix. After analyzing the geninue images and their recaptured counterparts, we noticed that recaptured images exhibit distinct characteristics. Resizing the images for modeling would have caused us to lose these critical details, which is also why we didn\u0026rsquo;t start with CNNs. The unique features of recaptured images arise from the printing process, where some original image details are lost and new features, like the texture of the print, are added.\nTherefore, we began research image processing techniques to extract these features precisely. We compared the resulting feature value distributions produced by each techniques and to decide the techniques to apply and which features to create.\nWe trained multiple clasifical models with the feature matrix, performing hyperparameter tuning and evaluatng the goodness of the fit to arrive at the final model that best distinguished between geninue image vs. recaptured images for the images we had.\nAlthough we attempted to use CNNs, the results were not significantly better in classifying the testing images. However this does not rule out the possibility that CNNs could eventually prove to be the best overall approach.\nCaveats #The project aims to showcase the potential of using the described method to detect photo of photos as a proof-of-concept. To develop a final model that can handle real-world scenarios, much more image data is needed for training. Additionally, the selected features may only be effective for the specific types of prints from the recaptured images in the training data.\nWe did found some features that showned potential for different types of recaptured images upon testing on various sources. However, I won\u0026rsquo;t be disclosing them.\nFinal Product #Besides the proof-of-concept machine learning model, we\u0026rsquo;ve also created a demo application using Dash in Python. It allows a human judge to process new images within the application and assess the results before making a final decision.\nThe application handles uploaded images, apply the necessary feature extraction, inputs them into the best model for prediction, and displays a confidence score in the app.\nWhat I\u0026rsquo;ve learned #Through this project, I gained extensive knowledge about various image processing techniques, and leaned how combining these techniques can help train a functional image classification machine learning model with limited data.\nReferences #1\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rScale-invariant feature transform (SIFT)\n2Plotly Dash\n","date":"30 Jun, 2022","permalink":"/projects/photo_photo/","section":"Projects","summary":"Using machine learning algorithms to detect Photo of a Photo","title":"Photo of Photo"},{"content":"\rIntroduction #This blog explores some of the applications of Monte Carlo Simulation.\nMonte Carlo Simulation #Monte Carlo Simulation or Monte Carlo methods/experients is a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle.\nIt is mostly use in problems such as optimization, numerical integration and generating draws from a probability distribution. This provides a very powerful tool in science, engineering, artificial intelligence, finance and cryptography etc espcially in risk assessment and model the probability of different outcomes in a process that cannot easily be predicted due to the intervention of random variables.\nHowever, its main draws back comes from users has to choose between accuracy and computational cost, the curse of dimensionality and the reliability of random number generators.\nThe overview of the Monte Carlo methods is as follow:\nDefine a domain of possible inputs Generate inputs randomly from a probability distribution over the domain Perform a deterministic computation of the outpus Aggregate the results Estimating Pi #This is the most used example for introduction to the Monte Carlo Simulation - to estimate the value of Pi. i.e. using Monte Carlo Simulation for numerical integration.\nIt is known that the circumference is: 2 Ï r hence Ï is the ratio of circumference and the 2 times radius (diameter) of a circle, and the area of a circle is: Ï r2. Where r is the radius of the circle. Therefore, the area of a circle of radius 1 is Ï.\nThe idea is to imagine a circle with radius 1 (i.e. diameter of 2) sitting inside a 2 x 2 shaped square. The ratio of the areas between the the circle and the square is $$ R = \\frac{\\text{area of circle}}{\\text{area of square}} = \\frac{\\pi r^2}{2 * 2} = \\frac{Ï}{4} $$\nHence if we have the ratio of the areas (R) between the circle and the square then we know that $$ \\pi = 4 * R = 4 * \\frac{\\text{area of circle}}{\\text{area of square}} $$\nOne may ask without knowing Ï how the area of the circle to be calculated then? This is where Monte Carlo Simulation comes in. We can first draw uniformed random samples of dots within the square then count the number of samples that fall within the circle inside the square. By calculating the number of samples fall inside the circle vs. total of random samples generated we can then estimate the value of Ï.\nDraw n random samples (x, y) coordinates uniformly within the bound of the square Collects the samples (x, y) that falls within the the circle (x2 + y2 \u0026lt;= r2 = 1) Calculate the ratio (R) of samples inside the circle vs n Multiply the ratio (R) by 4 to get the estimated value of Ï Click me to see the implementation in Python:\rStock Prices #Reference #","date":"10 May, 2022","permalink":"/blog/mcmc/","section":"Blog","summary":"Exploring Monte Carlo Simulation","title":"Monte Carlo Simulation"},{"content":"","date":null,"permalink":"/tags/statistics/","section":"Tags","summary":"","title":"Statistics"},{"content":"\rIntroduction #This project discusses the process of A/B testing on web analytics, using an example to determine if proposed improvements made to the website homepage impact the click-through rate to showcase the methodology.\nBackground #A/B testing is a form of statistical hypothesis testing, specifically two-sample hypothesis testing, conducted through a randomized experiment involving a control group and an experimental group. It aims to determine whether proposed changes (experiment variations) have a measurable impact compared to the original (control).\nTypically, A/B testing is used to assess differences between two samples: a control and an experimental group. However, this methodology can also extend to testing multiple samples simulataneously.\nThis process was discussed in the paper Improving Library User Experience with A/B Testing: Principles and Process by Scott W.H. Young (2014).\nThe original study aimed to address why the Interact category button has the lowest click-through rate among all categories on the Montana State University Library homepage. Various proposed variations, such as Connect, Learn, Help, and Services were tested via user survey to gather student suggestions on the changes. Then an A/B testing of multiple samples was conducted to evaluate the click-through rate of each variation and determine which one successfully increased the click-through rate.\nData #The data was collected based on 100% of website vistors during May 29, 2013 to June 18, 2013 using Google Analytics and Crazy Egg results to collect user\u0026rsquo;s various activities on the website. Then each of the variations was displayed to users randomly and the total number of clicks along with other activities was collected.\nThe Crazy Egg data can be found here.\nThe data consists of 5 csv files containing the number of clicks on each element on the webpage and corresponding to the 4 new suggested categories variations (Connect, Learn, Help, and Services) and the original (Interact) category.\nAfter consolidating the csv files and data manipulation, the summary of the clicks for each variation are as followed:\nWebpage Variation\rTotal Clicks\rHome Page Clicks\rAdjusted Clicks\rElement Clicks\rInteract\r3,714\r1,291\r2,423\r42\rConnect\r1,587\r83\r1,504\r53\rLearn\r1,652\r83\r1,569\r21\rHelp\r1,717\r122\r1,595\r38\rServices\r1,348\r49\r1,299\r45\rThe number of clicks on the home page is excluded from the total clicks to calculate the adjusted clicks to better captures the clicks on the elements after visitor landed on the home page. The element clicks indicates the number of clicks on each of the variations presented to site visitors.\nClick me to see the data manipulation in Python:\rimport os import glob import pandas as pd import re filepath = r\u0026#34;CrazyEgg\u0026#34; df_list = [] for subdir, dirs, files in os.walk(filepath): # Find all CSV files in the current directory csv_files = glob.glob(os.path.join(subdir, \u0026#34;*.csv\u0026#34;)) for file in csv_files: # Read the CSV file df = pd.read_csv(file) # Optionally, add a column to track the source file df[\u0026#34;source_file\u0026#34;] = file # Append the dataframe to the list df_list.append(df) # Concatenate all dataframes into one crazy_egg = pd.concat(df_list, ignore_index=True) # Extract filename using regex pattern = r\u0026#34;.*[\\\\/](?P\u0026lt;filename\u0026gt;[^\\\\/]+)$\u0026#34; crazy_egg[\u0026#34;source_file\u0026#34;] = crazy_egg[\u0026#34;source_file\u0026#34;].apply( lambda x: re.sub(pattern, r\u0026#34;\\1\u0026#34;, x) ) # Creating webpage ID column from filenames. webpage_pattern = r\u0026#34;(?\u0026lt;=- )\\w+\u0026#34; crazy_egg[\u0026#34;webpage\u0026#34;] = crazy_egg[\u0026#34;source_file\u0026#34;].apply( lambda x: re.search(webpage_pattern, x).group(0) if re.search(webpage_pattern, x) else None ) # Create base total click through number table total = crazy_egg[crazy_egg[\u0026#34;Snapshot information\u0026#34;].fillna(\u0026#34;\u0026#34;).str.contains(\u0026#34;created\u0026#34;)][ [\u0026#34;webpage\u0026#34;, \u0026#34;Snapshot information\u0026#34;] ] total.rename(columns={\u0026#34;Snapshot information\u0026#34;: \u0026#34;total clicks\u0026#34;}, inplace=True) click_pattern = r\u0026#34;\\d+(?= clicks)\u0026#34; total[\u0026#34;total clicks\u0026#34;] = total[\u0026#34;total clicks\u0026#34;].apply( lambda x: re.search(click_pattern, x).group(0) if re.search(click_patterh, x) else None ) total[\u0026#34;total clicks\u0026#34;] = total[\u0026#34;total clicks\u0026#34;].astype(int) # Create home page click through number home_clicks = crazy_egg[crazy_egg[\u0026#34;Name\u0026#34;].str.contains(\u0026#34;Home\u0026#34;)][ [\u0026#34;webpage\u0026#34;, \u0026#34;No. clicks\u0026#34;] ] home_clicks.rename(columns={\u0026#34;No. clicks\u0026#34;: \u0026#34;homepage clicks\u0026#34;}, inplace=True) home_clicks[\u0026#34;homepage clicks\u0026#34;] = home_clicks[\u0026#34;homepage clicks\u0026#34;].astype(int) # Create element click through number element_clicks = crazy_egg[crazy_egg[\u0026#34;Name\u0026#34;] == crazy_egg[\u0026#34;webpage\u0026#34;].str.upper()][ [\u0026#34;webpage\u0026#34;, \u0026#34;No. clicks\u0026#34;] ] element_clicks.rename(columns={\u0026#34;No. clicks\u0026#34;: \u0026#34;element clicks\u0026#34;}, inplace=True) element_clicks[\u0026#34;element clicks\u0026#34;] = element_clicks[\u0026#34;element clicks\u0026#34;].astype(int) # Combine create final click through number table click_through = pd.merge(total, home_clicks) click_through[\u0026#34;adjusted clicks\u0026#34;] = ( click_through[\u0026#34;total clicks\u0026#34;] - click_through[\u0026#34;homepage clicks\u0026#34;] ) click_through = pd.merge(click_through, element_clicks) click_through Click me to see the data manipulation in R:\rlibrary(datasets) library(tidyverse) filepath \u0026lt;- \u0026#34;CrazyEgg\u0026#34; # Getting the filenames of the .csv\u0026#39;s and put in a data frame. crazy_egg \u0026lt;- data.frame(filename = list.files( path = filepath, pattern = \u0026#34;*.csv\u0026#34;, full.names = TRUE, recursive = TRUE )) # Consolidate all csv files. crazy_egg$raw \u0026lt;- map(as.character(crazy_egg$filename), read_csv) # Regex pattern to extract filenames pattern \u0026lt;- \u0026#34;.*/([^/\\\\\\\\]+)$\u0026#34; crazy_egg$filename \u0026lt;- sub(pattern, \u0026#34;\\\\1\u0026#34;, crazy_egg$filename) # Creating webpage name from filenames (Interact, Connect, Learn, Help, Services). crazy_egg$webpage \u0026lt;- str_extract( string = crazy_egg$filename, pattern = \u0026#34;(?\u0026lt;=- )\\\\w+\u0026#34; ) # Unnesting data frames to make one big dataframe. crazy_egg \u0026lt;- unnest(crazy_egg) # Making Column names more readable. colnames(crazy_egg) \u0026lt;- make.names(colnames(crazy_egg)) # Extracting click count and create click-through base table. click_through \u0026lt;- filter(crazy_egg, grepl(\u0026#34;created\u0026#34;, Snapshot.information)) %\u0026gt;% select(webpage, Snapshot.information) colnames(click_through) \u0026lt;- c(\u0026#34;webpage\u0026#34;, \u0026#34;clicks\u0026#34;) click_through$clicks \u0026lt;- str_extract( string = click_through$clicks, pattern = \u0026#34;\\\\d+(?= clicks)\u0026#34; ) %\u0026gt;% as.numeric() # Extracting homepage clicks. click_through$home_page_clicks \u0026lt;- filter(crazy_egg, grepl(\u0026#34;Home\u0026#34;, Name)) %\u0026gt;% select(No..clicks) %\u0026gt;% unlist() # Homepage adjusted clicks. click_through \u0026lt;- mutate(click_through, adjusted_clicks = clicks - home_page_clicks) # Target clicks (e.g., Interact, Connect, Learn, Help, Services button). click_through$target_clicks \u0026lt;- filter(crazy_egg, grepl(paste(toupper(click_through$webpage), collapse = \u0026#34;|\u0026#34;), Name)) %\u0026gt;% select(No..clicks) %\u0026gt;% unlist() click_through \u0026lt;- mutate(click_through, click_rate = target_clicks / adjusted_clicks) click_through Visualization #\rClick me to see the code for the graphs in Python:\rimport plotly.express as px click_through[\u0026#34;click_through_rate\u0026#34;] = ( click_through[\u0026#34;element clicks\u0026#34;] / click_through[\u0026#34;adjusted clicks\u0026#34;] ) fig = px.bar( click_through, x=\u0026#34;webpage\u0026#34;, y=\u0026#34;click_through_rate\u0026#34;, text=click_through[\u0026#34;click_through_rate\u0026#34;].apply( lambda x: f\u0026#34;{x*100:.2f}%\u0026#34; ), # Format as percentage with two decimal places title=\u0026#34;Click-through Rate of Each Variation\u0026#34;, ) # Update layout to customize the plot fig.update_layout( title={ \u0026#34;text\u0026#34;: \u0026#34;Click-through Rate of Each Variation\u0026#34;, \u0026#34;y\u0026#34;: 0.9, \u0026#34;x\u0026#34;: 0.5, \u0026#34;xanchor\u0026#34;: \u0026#34;center\u0026#34;, \u0026#34;yanchor\u0026#34;: \u0026#34;top\u0026#34;, }, xaxis_title=\u0026#34;Webpage\u0026#34;, yaxis_title=\u0026#34;Click-through Rate (%)\u0026#34;, autosize=False, width=600, height=600, ) # Update y-axis tick labels to percentage format fig.update_yaxes( tickformat=\u0026#34;.1%\u0026#34;, range=[0, 0.04] ) # Tick format to display as percentage with two decimal places # Update hover template to show more decimal places fig.update_traces(hovertemplate=\u0026#34;Webpage: %{x}\u0026lt;br\u0026gt;Click-through Rate: %{y:.4f}\u0026#34;) # Show the figure fig.show() The graph shows the click-through rate of each webpage variation and an estimated 95% confidence interval assuming a Binomial Distribution where probability of success (p) is the click-through rate and (n) is the adjusted number of clicks on each variations.\nClick-through rate is defined by dividing the number of Element Clicks by the Adjusted Clicks. (i.e. percentage of total clicks results in a click on the element)\nAs we can see the Connect, Services variations both has much higher click-through rate than the original with Interact (2nd last). Also by assessing the 95% confidence intervals, we can get a sense already on if there are statistical differences between some of these click-through rates given if the confidence levels are overlapping or not.\nA/B Testing #Here, we conducted a pairwise hypothesis test with a significance level of Î± = 0.05 to determine which webpage variations show statistically significant differences in click-through rate. Given that we are comparing proportions, a pairwise proportion test was conducted.\nThe p-value of the test results are correct for multiple samples to minimize chances of Type I error (false positive) or incorrectly concluded that there are differences in the click-through rate when there isn\u0026rsquo;t. See here for the details on the types of errors.\nThe pair-wise proportions is conducted by comparing the click-through rate of each webpage variation against the other webpage variation.\nThe assumption of the hypothesis test (Null hypothesis) is: There are no differences in the click-through between the two webpage variations The alternative to the assumption (Alternative hypothesis) is: There are differences in the click-through between the two webpage variations\nClick me to see the implementation of the test in Python:\rimport statsmodels.stats.proportion as prop from statsmodels.stats.multitest import multipletests # Assigning names to successes and trials successes = click_through[\u0026#34;element clicks\u0026#34;].values trials = click_through[\u0026#34;adjusted clicks\u0026#34;].values webpages = click_through[\u0026#34;webpage\u0026#34;].values # Perform pairwise proportion tests results = [] for i in range(len(successes)): for j in range(i + 1, len(successes)): success1 = successes[i] trial1 = trials[i] success2 = successes[j] trial2 = trials[j] count = np.array([success1, success2]) nobs = np.array([trial1, trial2]) # Perform two independent binomial samples z_stat, p_value = prop.test_proportions_2indep( success1, trial1, success2, trial2 ) # Store results result = { \u0026#34;Webpage A\u0026#34;: webpages[i], \u0026#34;Webpage B\u0026#34;: webpages[j], \u0026#34;z_statistic\u0026#34;: z_stat, \u0026#34;p_value\u0026#34;: p_value, } results.append(result) # Convert results to DataFrame results_df = pd.DataFrame(results) # Adjust p-values using Bonferroni correction p_adjusted_bonf = multipletests(results_df[\u0026#34;p_value\u0026#34;], method=\u0026#34;bonferroni\u0026#34;)[1] results_df[\u0026#34;p_adjusted_bonf\u0026#34;] = p_adjusted_bonf # Adjust p-values using Benjamini-Hochberg (BH) correction p_adjusted_bh = multipletests(results_df[\u0026#34;p_value\u0026#34;], method=\u0026#34;fdr_bh\u0026#34;)[1] results_df[\u0026#34;p_adjusted_bh\u0026#34;] = p_adjusted_bh # Display results results_df[\u0026#34;test_result\u0026#34;] = results_df[\u0026#34;p_value\u0026#34;].apply( lambda p: \u0026#34;Reject\u0026#34; if p \u0026lt; 0.05 else \u0026#34;Failed to reject\u0026#34; ) results_df[\u0026#34;test_result_adjusted_bonf\u0026#34;] = results_df[\u0026#34;p_adjusted_bonf\u0026#34;].apply( lambda p: \u0026#34;Reject\u0026#34; if p \u0026lt; 0.05 else \u0026#34;Failed to reject\u0026#34; ) results_df[\u0026#34;test_result_adjusted_bh\u0026#34;] = results_df[\u0026#34;p_adjusted_bh\u0026#34;].apply( lambda p: \u0026#34;Reject\u0026#34; if p \u0026lt; 0.05 else \u0026#34;Failed to reject\u0026#34; ) results_df Click me to see the implementation of the test in R:\rsuccesses \u0026lt;- click_through$element_clicks trials \u0026lt;- click_through$adjusted_clicks names(successes) \u0026lt;- click_through$webpage names(trials) \u0026lt;- click_through$webpage # Pair-wise proportion test with Bonferroni Correction pairwise.prop.test(successes, trials, p.adjust.method = \u0026#34;bonferroni\u0026#34; ) # Pair-wise proportion test with Benjamini-Hochberg (BH) Correction pairwise.prop.test(successes, trials, p.adjust.method = \u0026#34;BH\u0026#34; ) Results #\rWebpage A\rWebpage B\rz_statistic\rp-value\rp-value (Bonf. adj.)\rp-value (BH. adj.)\rTest Result\rTest Result (Bonf. adj.)\rTest Result (BH. adj.)\rLearn\rInteract\r-0.932\r0.350\r1.000\r0.389\rFailed to reject null\rFailed to reject null\rFailed to reject null\rLearn\rHelp\r-2.139\r0.032\r0.324\r0.064\rReject null\rFailed to reject null\rFailed to reject null\rLearn\rServices\r-3.609\r0.000\r0.003\r0.002\rReject null\rReject null\rReject null\rLearn\rConnect\r-3.879\r0.000\r0.001\r0.001\rReject null\rReject null\rReject null\rInteract\rHelp\r-1.423\r0.155\r1.000\r0.193\rFailed to reject null\rFailed to reject null\rFailed to reject null\rInteract\rServices\r-3.050\r0.000\r0.023\r0.006\rReject null\rReject null\rReject null\rInteract\rConnect\r-3.301\r0.001\r0.010\r0.003\rReject null\rReject null\rReject null\rHelp\rServices\r-1.705\r0.088\r0.882\r0.126\rFailed to reject null\rFailed to reject null\rFailed to reject null\rHelp\rConnect\r-1.858\r0.063\r0.631\r0.105\rFailed to reject null\rFailed to reject null\rFailed to reject null\rServices\rConnect\r-0.071\r0.943\r1.000\r0.943\rFailed to reject null\rFailed to reject null\rFailed to reject null\rTotal of 10 pair-wise proportion tests were conducted and the test statistics (z_statistic) and the p-value from the test were calculated.\nThen adjustments on the p-values were applied using the Bonferroni Correction and Benjamini-Hochberg Procedure methods were applied. The adjusted p-values are shown in the table p-value (Bonf. adj.) and p-value (BH. adj.).\nAt Î± = 0.05 significant level, the pair-wise proportion tests with multiple samples adjustments shows that there are statistical significance between Learn and Connect and Learn and Connect variations.\nMore importantly, both Connect and Services variations are significantly different to the original Interact variation. Also the differences between Connect and Services has no statistical differences based on the test results. The test results aligns with what we saw earlier from the confidence intervals of the click-through rate.\nIn other words, both Connect and Services would be a good candidate to replace the original Interact button for improving the user engagement in navigating to the detail contents after landing on the home page.\nThe original paper also explored the Drop-off rate (% of users leave the site from the category page) and Homepage-return rate (% of users returns to the homepage from the category page) for each of these variations as well. Which are also important beside click-through rate, as these metrics also provide further evidence that the proposed changes increases user engagement to the category page.\nReferences #A/B Testing\nPair-wise Proportion Test\nType of Errors\nBonferroni Correction\nBenjamini-Hochberg\nImproving Library User Experience with A/B Testing: Principles and Process (Young, 2014)\n","date":"02 Apr, 2022","permalink":"/projects/ab_testing/","section":"Projects","summary":"Process of A/B testing for web analytics","title":"A/B Testing"},{"content":"","date":null,"permalink":"/tags/data-analytics/","section":"Tags","summary":"","title":"Data Analytics"},{"content":"","date":null,"permalink":"/tags/hypothesis-testing/","section":"Tags","summary":"","title":"Hypothesis Testing"},{"content":"","date":null,"permalink":"/tags/r/","section":"Tags","summary":"","title":"R"},{"content":"","date":null,"permalink":"/tags/web-analytics/","section":"Tags","summary":"","title":"Web Analytics"},{"content":"","date":null,"permalink":"/tags/linear-algebra/","section":"Tags","summary":"","title":"Linear Algebra"},{"content":"\rLaunch App\rIntroduction #The aim of this project is to provide an educational tool that enhances understanding of Polynomial Regression1, model complexity, and the trade-offs between model fit and over-fitting in data analysis and predictive modeling.\nThe final product is an interactive app built using R Shiny. This application will allow users to explore polynomial regression by adjusting the degree of the polynomial along with other inputs to observe real-time updatesto the regression line, residual plots and key statistical metrics.\nBackground #During my undergraduate studies, I was fascinated by the numerical methods used to find approximate solutions for problems where exact solutions are difficult to obtain. These methods form the backbone of many machine learning algorithms, such as numerical linear algebra2 for solving regression problems with large datasets, optimization algoithm like gradient descent3 for minimizing cost functions, and backpropagation4 for solving complex chained derivatives when training neural networks5. This fascination inpired me to create an interactive tool that demonstrates these principles in action.\nBuilding the App #The application is built using R Shiny, a powerful tool beloved by R enthusiasts and those interested in creating interactive dashboards using the wide range of statistical modeling packages native in R.\nR Shiny simplifies deployment to local environments with minimal effort, although mastering its UI syntax requires some learning, unlike tools such as Dash that supports syntax similar to HTML in Python.\nThe source code can be found here at GitHub.\nHow to use the App #User Inputs:\nInput expression for the function f(x) to generate sample data points for the application Adjust the range of input values for x in Value Range for x input box User can increase or decrease the Number of Points to be generated Set Seed for Noise RNG for generating the same set of random points Adjust the Variance Level for Noise in the data - higher variance makes the data noisier and appear more random Max Polynomial Degree Fitted controls the maximum degree of the polynomial used. Click the tiny Play button to visualize animated changes as higher polynomial degrees are added Plot X Axis Range to change the area of the plot to be viewed based on the ranges of the x values Tabs:\nPlot\nUser inputs and the main plot area for visualization as well as the regression summary details Residual Plots\nResidual plots of the model predictions vs actual values as higher degree polynomials is added Data\nAll raw input and model prediction values in a table Conclusion #Thank you for reading! If you like to learn more details about Polynomial Regression1 you can also check out this article I wrote on Medium Polynomial Regressionã°ï¸ and play with the app as you read it or leave a comment below (GitHub required).\nReferences #1\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rPolynomial Regression 2\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rNumerical Linear Algebra 3\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rGradient Descent 4\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rBackpropagation 5\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rNeural Networks\n","date":"06 Feb, 2021","permalink":"/projects/polyfit/","section":"Projects","summary":"Interactive R Shiny App to Visualize Polynomial Regression","title":"Polyfitã°ï¸"},{"content":"","date":null,"permalink":"/tags/predictive-modeling/","section":"Tags","summary":"","title":"Predictive Modeling"},{"content":"","date":null,"permalink":"/tags/r-shiny/","section":"Tags","summary":"","title":"R Shiny"},{"content":"Hello internet, welcome to my personal website ð£! First time doing this, let\u0026rsquo;s see how it goes!\nDisplay some Math: #Einstein\u0026rsquo;s famous Mass-energy equivalence formula ð:\n$$ E = mc^2 $$\nFrom wiki: The formula defines the energy E of a particle in its rest frame as the product of mass m with the speed of light squared (cÂ²).\nThe ð Curve: With the probability density function:\n$$ f(x | Î¼, Ï^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(x - Î¼)^2}{2 \\sigma^2} \\right) $$\nWhere Î¼ is the mean or expectation of the distribution and Ï is the standard deviation. i.e. variance of ÏÂ². It is also known as the Gaussian Distribution or more commonly known as the Normal Distribution. An example graph of the function:\nIllustration of the Probability Density Function of a Standard Normal Distribution\rEmbedding code block #This is the R code used to generate the above graph.\n#Clear console and environment rm(list = ls()) cat(\u0026#34;\\014\u0026#34;) #attach required library library(ggplot2) library(dplyr) #set mu and sigma parameters for the Normal Curve mu\u0026lt;-0 sigma\u0026lt;-1 #generate data x\u0026lt;-seq(-4*sigma,4*sigma,0.0001)-0.5 y\u0026lt;-dnorm(x,mu,sigma) #density function data\u0026lt;-cbind(x=x,y=y) %\u0026gt;% data.frame() xlab\u0026lt;-c(expression(-3*sigma) ,expression(-2*sigma) ,expression(-sigma) ,expression(mu) ,expression(sigma) ,expression(2*sigma) ,expression(3*sigma) ) p_lab\u0026lt;-pnorm(seq(-3*sigma,3*sigma,sigma),mu,sigma) #Plot: ggplot(data,aes(x,y))+ #Plot area settings: theme_classic()+ theme(plot.title=element_text(size=40,face=\u0026#34;bold\u0026#34;,hjust=0.5) ,panel.grid.major.x=element_line(size = (0.2)) ,axis.text.x=element_blank())+ ggtitle(\u0026#34;Standard Normal Distribution\u0026#34;)+ xlab(\u0026#34;x\u0026#34;)+ #text for the sigma labels annotate(\u0026#34;text\u0026#34;, x = seq(-3*sigma,3*sigma,sigma), y = rep(-0.005,7), label = xlab, family = \u0026#34;\u0026#34;, fontface = 3, size=8) + #text for the density values at each n*sigma area annotate(\u0026#34;text\u0026#34;, x = c(seq(-3*sigma,3*sigma,sigma)-0.5*sigma,0.5*sigma+3*sigma), y = round(c(p_lab[1],diff(p_lab),p_lab[1]),1)^2+0.05, label = paste0(round(c(p_lab[1],diff(p_lab),p_lab[1])*100,1),\u0026#34;%\u0026#34;), family = \u0026#34;\u0026#34;, fontface = 8, size=8) + #display parameter values annotate(\u0026#34;text\u0026#34;, x = rep(2.2*sigma,2), y = c(0.16,0.14), label = c(expression(paste(mu,\u0026#34;=\u0026#34;)), expression(paste(sigma,\u0026#34;=\u0026#34;))), family = \u0026#34;\u0026#34;, fontface = 3, size=8) + annotate(\u0026#34;text\u0026#34;, x = rep(2.4*sigma,2), y = c(0.164,0.143), label = c(mu,sigma), family = \u0026#34;\u0026#34;, fontface = 3, size=7) + scale_x_continuous(breaks=seq(-3*sigma,3*sigma,sigma),limits=c(-3.5*sigma,3.5*sigma))+ ylab(\u0026#34;Probability Density\u0026#34;)+ scale_y_continuous(labels=scales::percent_format(accuracy=1))+ geom_line() Some Linear Algebra: #To find a plane of best-fit. Given a data vector with n samples and p parameters:\n$$ \\begin{Bmatrix} y_i, x_{i1},\\cdot \\cdot \\cdot ,x_{ip} \\end{Bmatrix}^{n}_{i=1} $$\nWhere the dependent variable y and the p-size vector of regressors x are assumed to be a linear relationship. Where the error variable Îµ was modeled such that it is the minimum and ideally it experiences an unobserved random variable. i.e. \u0026ldquo;noise\u0026rdquo;.\nThen ideally we want to find Î² where the model has the form:\n$$ y_i = \\beta_0 + \\beta_1 x_1 + \\cdot \\cdot \\cdot + \\beta_p x_{ip} + \\varepsilon_i = x^T \\beta + \\varepsilon $$\nOr simply\n$$ y = X \\beta + \\varepsilon $$\nWhere\n$$ y = \\begin{Bmatrix} y_1, \\cdot \\cdot \\cdot y_n \\end{Bmatrix}^T $$\n$$ X = \\begin{pmatrix} x^T_1 \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ x^T_n \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; x_{11} \u0026amp; \\cdot\\cdot\\cdot \u0026amp; x_{1p} \\\\\\ \u0026amp; \u0026amp; \\cdot \u0026amp; \\\\\\ \u0026amp; \u0026amp; \\cdot \u0026amp; \\\\\\ \u0026amp; \u0026amp; \\cdot \u0026amp; \\\\\\ 1 \u0026amp; x_{n1} \u0026amp; \\cdot\\cdot\\cdot \u0026amp; x_{np} \\end{pmatrix} $$\nand\n$$ \\beta = \\begin{pmatrix} \\beta_0 \\\\\\ \\beta_1 \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\beta_p \\end{pmatrix} , \\varepsilon = \\begin{pmatrix} \\varepsilon_1 \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\varepsilon_n \\end{pmatrix} $$\nHence for a line, you are solving for the equation with one parameter $$y = \\beta_0 + \\beta_1 x$$ and for a plane there will be two parameters. $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$$\nLeast-squares estimation: #Since y and x are assumed to be a linear relationship and we would like to find the \u0026ldquo;best\u0026rdquo; Î² which solve the system of equations and to minimize Îµ. Hence let: $$ \\varepsilon = L(X,y,\\hat{\\beta})=\\left | X\\hat{\\beta}-y \\right |^2 = (X\\hat{\\beta} - y)^T(X\\hat{\\beta}-y) \\ $$ $$ = y^Ty - y^TX\\hat{\\beta} - \\hat{\\beta}^TX^Ty + \\hat{\\beta}^TX^TX\\hat{\\beta} $$ Where L is called the Loss function, essentially the error term is modeled with the input X, y, Î². Since X, y is the original data we want to \u0026ldquo;fit\u0026rdquo;, therefore we can find the \u0026ldquo;best\u0026rdquo; Î² at which L(X, y, Î²) is minimized.\nHence the first derivative of L(X, y, Î²):\n$$ \\frac{\\partial L(X,y,\\hat{\\beta})}{\\partial \\hat{\\beta}} = -2X^Ty+2X^TX\\hat{\\beta} $$\nSince X, y is fixed and known and we only interested in the \u0026ldquo;best\u0026rdquo; Î² therefore:\n$$ \\hat{\\beta} = (X^TX)^{-1}X^Ty $$\nThis is the case for the Simple Linear Regression.\nThis concludes the Hello World! ð Thank you for reading!\n","date":"24 Jan, 2021","permalink":"/blog/hello_world/","section":"Blog","summary":"Hello internet, welcome to my personal website ð£!","title":"Hello World! ð"},{"content":"\rA picture of me - Jun, 2024\rHi! ä½ å¥½! ð\nMy name is Steven Lio. I was born in Hong Kong, China and grew up in Macau, China. I moved to Canada in 2007 and currently living in Toronto, Ontario.\nI received my Master of Data Science from University of British Columbia (Canada) and my Bachelor of Science degree in Statistics from McMaster University (Canada).\nWith over 6 years of professional experience, I specialized in developing data-driven solutions and workflows to extract actionable insights that support strategic decision-making for cross-functional teams in Government, Telecom, Retail and Software industries. I possess a natural curiousity and a keen eye for detail when analyzing data, enabling me to find the optimal solutions for finding data-driven solutions and drive business decisions.\nProficient in Python and R, I excel in creating end-to-end machine learning workflows for data science projects. My expertise includes developing efficient data pipelines, conducting feature engineering, implementing machine learning models and building interactive visualization applications. Like this one here.\nI have practical experience tackling real-world challenges by applying machine learning algorithms for a variety of tasks. These include classification, clustering, statistical inference, regression modeling, time series modeling, all aimed at enhancing analytical insights and decision-making process. I have also developed advanced machine learning models such as implementing and training neural networks (CNN) for image classification, segmentation, computer vision tasks. My expertise extends to writing complex SQL queries for processing and aggregating large datasets, and designing robust data ETL processes.\nMy advanced data analytics have supported marketing strategy research in areas such as Predicting Customer Value, Customer Segmentation, Sentiment Analysis, Market Potential and Shares Estimation, Competitor Analysis, Optimizing Sales Channels.\nSomething else about me #On my free time, I enjoyed roaming in the nature and photography. I loves animals and I have a cat named \u0026ldquo;HÇHÇ\u0026rdquo; (Tiger in Mandarin Chinese). I also play the piano and guitar. My favourites composers are Chopin and Beethoven.\nIf you are interested to learn more about me, feel free to message me on LinkedIn, or follow me on Instagram.\nFavourite quotes: #\u0026ldquo;MEOW~\u0026rdquo; - HuHu\n\u0026ldquo;All models are wrong, but some are useful\u0026rdquo; - George Box\n\u0026ldquo;We are the representatives of the cosmos; we are an example of what hydrogen atoms can do, given 15 billion years of cosmic evolution.\u0026rdquo; - Carl Sagan\nCheck out the projects I\u0026rsquo;ve previously worked on:\nLearn More\r","date":null,"permalink":"/about/","section":"","summary":"A picture of me - Jun, 2024\rHi!","title":"About me"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"Technical Skills # Programming Languages: Python\r, R\r, SQL\rC#\r, VB.NET\r, .Net Development\rHtml\r, CSS\rMachine Learning Libraries: PyTorch\r, TensorFlow\r, scikit-learn\r, SciPy\r, pandas\r, numpy\r, OpenCV\r, Pillow\rData Visualization: Dash\r, Shiny\r, Plotly\r, Vega-Altair\r, matplotlib\r, ggplot\rTableau\r, PowerBI\rSoftware Development: Git\r, Github\r, Jira\r, Jenkins\r, Docker\r, PowerShell\r, bash\rOther tools: Microsoft SQL Server\r, Visual Studio\rArcGIS\r, QGIS\rSAS EG\r, MATLAB\r, MicroStrategy\rPower Automate Desktop\r, UiPath\r, Automation 360\r, BluePrism\rAnalytics topics: Customer Value Prediction\r, Customer Segmentation\r, Sentiment Analysis\rSeaonality Analysis\r, Market Potential and Shares Estimation\r, Competitor Analysis\rPre \u0026 Post Campaign Analysis\r, Optimizing Sales Channel\rProfessional Experience #\rData Scientist Â· Blueprint Software System\nSep, 2022 â Jun, 2024\nQuality Assurance Business Analyst Â· Blueprint Software System\nApr, 2021 â Aug, 2022\nData Scientist Â· Bell Mobility\nJan, 2020 â Dec, 2020\nMarket Insight Analyst Â· Bell Mobility\nAug, 2017 â Dec, 2019\nData QA Analyst Â· Environment and Climate Change Canada\nMay, 2016 â Apr, 2017\nCo-op / Internship #\rData QA Support Â· Environment and Climate Change Canada\nMay, 2015 â Dec, 2015\nAssistant Methodologist Â· Statistics Canada\nJan, 2014 â Aug, 2014\nProjects #Image Segmentation of Coral Baby Â· Personal Project\nJan, 2023 - Jun, 2023\nDeveloped an image segmentation model using a U-Net architecture with ResNet backbone, transfer learning using pre-trained weights from ImageNet to highlight area of coral babies in images of coral frags Photo of Photo Â· Trusting Pixels Inc.\nMay, 2022 - Jun, 2022\nCollaborated on a team to develop a proof-of-concept machine learning model to detect recaptured images Built an interactive Dash application incorporating the best model, allowing users to annotate and identify false recaptured images Project Page Polyfit Â· Personal Project\nFeb, 2021 - Feb, 2021\nInteractive R Shiny tool for visualizing curve fitting using polynomials Project Page, GitHub Launch App\rEducation #\rMasters of Data Science\nUniversity of British Columbia\nSep, 2021 - Aug, 2022\nHonors Bachelor of Science - Mathematics and Statistics Co-op\nMcMaster University\nSep, 2011 - Aug, 2016\nMisc. Information # Languages: English, Cantonese, Mandarin Download RÃ©sumÃ©\r","date":null,"permalink":"/resume/","section":"","summary":"Technical Skills # Programming Languages: Python\r, R\r, SQL\rC#\r, VB.","title":"RÃ©sumÃ©"}]
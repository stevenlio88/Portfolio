[{"content":"\rI specialize in developing tools and implementing advanced Machine Learning solutions to tackle challenges within Data Science and Data-Driven Analytics.\nLearn More\r","date":null,"permalink":"/","section":"","summary":"I specialize in developing tools and implementing advanced Machine Learning solutions to tackle challenges within Data Science and Data-Driven Analytics.","title":""},{"content":"Random things I wrote.\n","date":null,"permalink":"/blog/","section":"Blog","summary":"Random things I wrote.","title":"Blog"},{"content":"","date":null,"permalink":"/tags/data-science/","section":"Tags","summary":"","title":"Data Science"},{"content":"Hello internet, welcome to my personal website üê£! First time doing this, let\u0026rsquo;s see how it goes!\nDisplay some Math: #Einstein\u0026rsquo;s famous Mass-energy equivalence formula üåå:\n$$ E = mc^2 $$\nFrom wiki: The formula defines the energy E of a particle in its rest frame as the product of mass m with the speed of light squared (c¬≤).\nThe üîî Curve: With the probability density function:\n$$ f(x | Œº, œÉ^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(x - Œº)^2}{2 \\sigma^2} \\right) $$\nWhere Œº is the mean or expectation of the distribution and œÉ is the standard deviation. i.e. variance of œÉ¬≤. It is also known as the Gaussian Distribution or more commonly known as the Normal Distribution. An example graph of the function:\nIllustration of the Probability Density Function of a Standard Normal Distribution\rEmbedding code block #This is the R code used to generate the above graph.\n#Clear console and environment rm(list = ls()) cat(\u0026#34;\\014\u0026#34;) #attach required library library(ggplot2) library(dplyr) #set mu and sigma parameters for the Normal Curve mu\u0026lt;-0 sigma\u0026lt;-1 #generate data x\u0026lt;-seq(-4*sigma,4*sigma,0.0001)-0.5 y\u0026lt;-dnorm(x,mu,sigma) #density function data\u0026lt;-cbind(x=x,y=y) %\u0026gt;% data.frame() xlab\u0026lt;-c(expression(-3*sigma) ,expression(-2*sigma) ,expression(-sigma) ,expression(mu) ,expression(sigma) ,expression(2*sigma) ,expression(3*sigma) ) p_lab\u0026lt;-pnorm(seq(-3*sigma,3*sigma,sigma),mu,sigma) #Plot: ggplot(data,aes(x,y))+ #Plot area settings: theme_classic()+ theme(plot.title=element_text(size=40,face=\u0026#34;bold\u0026#34;,hjust=0.5) ,panel.grid.major.x=element_line(size = (0.2)) ,axis.text.x=element_blank())+ ggtitle(\u0026#34;Standard Normal Distribution\u0026#34;)+ xlab(\u0026#34;x\u0026#34;)+ #text for the sigma labels annotate(\u0026#34;text\u0026#34;, x = seq(-3*sigma,3*sigma,sigma), y = rep(-0.005,7), label = xlab, family = \u0026#34;\u0026#34;, fontface = 3, size=8) + #text for the density values at each n*sigma area annotate(\u0026#34;text\u0026#34;, x = c(seq(-3*sigma,3*sigma,sigma)-0.5*sigma,0.5*sigma+3*sigma), y = round(c(p_lab[1],diff(p_lab),p_lab[1]),1)^2+0.05, label = paste0(round(c(p_lab[1],diff(p_lab),p_lab[1])*100,1),\u0026#34;%\u0026#34;), family = \u0026#34;\u0026#34;, fontface = 8, size=8) + #display parameter values annotate(\u0026#34;text\u0026#34;, x = rep(2.2*sigma,2), y = c(0.16,0.14), label = c(expression(paste(mu,\u0026#34;=\u0026#34;)), expression(paste(sigma,\u0026#34;=\u0026#34;))), family = \u0026#34;\u0026#34;, fontface = 3, size=8) + annotate(\u0026#34;text\u0026#34;, x = rep(2.4*sigma,2), y = c(0.164,0.143), label = c(mu,sigma), family = \u0026#34;\u0026#34;, fontface = 3, size=7) + scale_x_continuous(breaks=seq(-3*sigma,3*sigma,sigma),limits=c(-3.5*sigma,3.5*sigma))+ ylab(\u0026#34;Probability Density\u0026#34;)+ scale_y_continuous(labels=scales::percent_format(accuracy=1))+ geom_line() Some Linear Algebra: #To find a plane of best-fit. Given a data vector with n samples and p parameters:\n$$ \\begin{Bmatrix} y_i, x_{i1},\\cdot \\cdot \\cdot ,x_{ip} \\end{Bmatrix}^{n}_{i=1} $$\nWhere the dependent variable y and the p-size vector of regressors x are assumed to be a linear relationship. Where the error variable Œµ was modeled such that it is the minimum and ideally it experiences an unobserved random variable. i.e. \u0026ldquo;noise\u0026rdquo;.\nThen ideally we want to find Œ≤ where the model has the form:\n$$ y_i = \\beta_0 + \\beta_1 x_1 + \\cdot \\cdot \\cdot + \\beta_p x_{ip} + \\varepsilon_i = x^T \\beta + \\varepsilon $$\nOr simply\n$$ y = X \\beta + \\varepsilon $$\nWhere\n$$ y = \\begin{Bmatrix} y_1, \\cdot \\cdot \\cdot y_n \\end{Bmatrix}^T $$\n$$ X = \\begin{pmatrix} x^T_1 \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ x^T_n \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; x_{11} \u0026amp; \\cdot\\cdot\\cdot \u0026amp; x_{1p} \\\\\\ \u0026amp; \u0026amp; \\cdot \u0026amp; \\\\\\ \u0026amp; \u0026amp; \\cdot \u0026amp; \\\\\\ \u0026amp; \u0026amp; \\cdot \u0026amp; \\\\\\ 1 \u0026amp; x_{n1} \u0026amp; \\cdot\\cdot\\cdot \u0026amp; x_{np} \\end{pmatrix} $$\nand\n$$ \\beta = \\begin{pmatrix} \\beta_0 \\\\\\ \\beta_1 \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\beta_p \\end{pmatrix} , \\varepsilon = \\begin{pmatrix} \\varepsilon_1 \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\varepsilon_n \\end{pmatrix} $$\nHence for a line, you are solving for the equation with one parameter $$y = \\beta_0 + \\beta_1 x$$ and for a plane there will be two parameters. $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$$\nLeast-squares estimation: #Since y and x are assumed to be a linear relationship and we would like to find the \u0026ldquo;best\u0026rdquo; Œ≤ which solve the system of equations and to minimize Œµ. Hence let: $$ \\varepsilon = L(X,y,\\hat{\\beta})=\\left | X\\hat{\\beta}-y \\right |^2 = (X\\hat{\\beta} - y)^T(X\\hat{\\beta}-y) \\ $$ $$ = y^Ty - y^TX\\hat{\\beta} - \\hat{\\beta}^TX^Ty + \\hat{\\beta}^TX^TX\\hat{\\beta} $$ Where L is called the Loss function, essentially the error term is modeled with the input X, y, Œ≤. Since X, y is the original data we want to \u0026ldquo;fit\u0026rdquo;, therefore we can find the \u0026ldquo;best\u0026rdquo; Œ≤ at which L(X, y, Œ≤) is minimized.\nHence the first derivative of L(X, y, Œ≤):\n$$ \\frac{\\partial L(X,y,\\hat{\\beta})}{\\partial \\hat{\\beta}} = -2X^Ty+2X^TX\\hat{\\beta} $$\nSince X, y is fixed and known and we only interested in the \u0026ldquo;best\u0026rdquo; Œ≤ therefore:\n$$ \\hat{\\beta} = (X^TX)^{-1}X^Ty $$\nThis is the case for the Simple Linear Regression.\nThis concludes the Hello World! üåé Thank you for reading!\n","date":"24 Jun, 2024","permalink":"/blog/hello_world/","section":"Blog","summary":"Hello internet, welcome to my personal website üê£!","title":"Hello World! üåé"},{"content":"","date":null,"permalink":"/tags/linear-algebra/","section":"Tags","summary":"","title":"Linear Algebra"},{"content":"","date":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning"},{"content":"Interesting projects I\u0026rsquo;ve done in the past.\n","date":null,"permalink":"/projects/","section":"Projects","summary":"Interesting projects I\u0026rsquo;ve done in the past.","title":"Projects"},{"content":"","date":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":"","date":null,"permalink":"/tags/r/","section":"Tags","summary":"","title":"R"},{"content":"I am working on putting my projects here. Stay tuned and I hope you will visit back later! Meanwhile, take a look at my R√©sum√© or check out my first blog here.\nHere is me working on it\u0026hellip;.\nLorem ipsum #Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nLorem ipsum #Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n","date":"24 Jun, 2024","permalink":"/projects/stay_tuned/","section":"Projects","summary":"More projects is being curated here . . .","title":"Stay tuned"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"Computer Vision"},{"content":"","date":null,"permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"Deep Learning"},{"content":"","date":null,"permalink":"/tags/image-processing/","section":"Tags","summary":"","title":"Image Processing"},{"content":"\rPicture taken at Honolulu, HI in Apr, 2024 ¬© Steven Lio\rIntroduction #The primary objective of this project is to create image mask accurately to hightlight the areas of baby corals growth from images of coral plugs. This will aid in the research of understanding growth patterns of baby corals, collect data on various parameters such as the size, dry weight, and growth / mortality rates in baby corals to evaluate the effectiveness of some treatments designed to enhance coral growth.\nCoral protection is essential for maintaining biodiversity in the marine ecosystems and significantly impacts global food production.\nHaving visited Honolulu, HI many times, I personally witnessed the devastating impact of ocean warming on coral bleaching and the consequent decline in marine life diversity.\nBackground #Image Segmentation1 is a well-studied topic in computer visions research long before the advancement in modern-days GPUs technology (notably Nvidia) that have enabled the development and application of more complex algorithms, such as Convolution Neural Networks (CNNs)2 for computer vision tasks.\nHowever, developing and training a CNN is very costly, and more importantly, requires a significant amount of representative training data to achieve accurate and reliable results.\nFor this project, I used an image set of coral plugs (total 130 images), consisting of top-down views of various individual circular coral plugs. Due to confidentiality, I cannot disclose the data source.\nHowever, I can show you an example of the images I used:\nImage example\rCoral baby (Middle)\rPreprocessing Images #The coral plug images were captured from a systems that was calibrated to centered the coral plugs in the image. However, there are variances where some are not as centered. The images captured were also from the same distance, each coral plug will appears to be nearly identical in size.\nThe images were first preprocessed by centering them and cropping the key area (i.e. coral plug) to ensure when developing an algorithm, the machine learning algorithm focuses only on the coral plug, minimizing distractions.\nTo achieve this, I used the Hough Circle Transform3 algorithm which is specifically designed for detecting \u0026ldquo;circles\u0026rdquo; in an image. This is ideal for my images as it helps locate the coral plug, find its center and crop out the excess areas.\nClick me to see the implementation in Python:\r# OpenCV library import cv2 # Path for image and output folder img_path = r\u0026#34;coral.jpg\u0026#34; out_path = r\u0026#34;out_folder\u0026#34; # Read image and resize to 512 x 512 img = cv2.imread(img_path) img = cv2.resize(img, (512, 512)) # Create a copy of the original image and a gray scale version of the image output = img.copy() gray = cv2.cvtColor(output, cv2.COLOR_BGR2GRAY) gray = cv2.medianBlur(gray, 5) # Padding the correct circle padding = 8 # Find Hough Circles circles = cv2.HoughCircles( gray, cv2.HOUGH_GRADIENT, dp=1.2, minDist=120, param1=100, param2=30, minRadius=120, maxRadius=180, ) # Draw Hough Circles try: circles = np.round(circles[0, :]).astype(\u0026#34;int\u0026#34;) for (x, y, r) in circles: if 200 \u0026lt; x \u0026lt; 250 and 200 \u0026lt; y \u0026lt; 250: cv2.circle(output, (x, y), r + padding, (0, 255, 0), 4) print(x, y, r) else: cv2.circle(output, (x, y), r, (0, 0, 255), 2) except: print(\u0026#34;No circles is found! Check Hough Circle parameters.\u0026#34;) # Save the final output to folder cv2.imwrite(\u0026#34;{0}\\{1}\u0026#34;.format(out_path, \u0026#34;houghcircles.jpg\u0026#34;), output) The output after applying Hough Circle Transform:\nIn the output image, correctly detected circles are marked in green, while incorrect ones are in red. Knowing the size of the image and the coral plug\u0026rsquo;s scale allowed me to constrain the Hough Circle algorithm to a specific radius range and focus on the center of the image. After identifying the key area, I cropped out this region and create a black mask to cover the remaining areas outside of the coral plug.\nClick me to see the final implementation in Python:\rdef find_plug(img, padding=5, create_mask=True, out_size=(512, 512)): \u0026#34;\u0026#34;\u0026#34; Find Coral Plug in image using Hough Circle. Args: img (numpy.ndarray): Input image containing coral plug. padding (int): Padding for the correct circle. create_mask (bool): Create a mask as the output. out_size (tuple): Output image size. Returns: int: The product of a and b. \u0026#34;\u0026#34;\u0026#34; # Gray scaling image gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) gray = cv2.medianBlur(gray, 5) # Define Hough Circles with hyper-parameters circles = cv2.HoughCircles( gray, cv2.HOUGH_GRADIENT, dp=1.2, minDist=120, param1=100, param2=30, minRadius=120, maxRadius=180, ) # Find the correct circle if circles is not None: circles = np.round(circles[0, :]).astype(\u0026#34;int\u0026#34;) for (x, y, r) in circles: if 200 \u0026lt; x \u0026lt; 250 and 200 \u0026lt; y \u0026lt; 250: r += padding break else: x, y, r = img.shape[0] // 2, img.shape[1] // 2, 80 break # Create the square cropping area based on the found circle cropped_image = img[y - r : y + r, x - r : x + r] # Mask out the areas outside of the circle if create_mask: mask = np.zeros_like(cropped_image) center = cropped_image.shape[0] // 2 cv2.circle(mask, (center, center), r + padding, (255, 255, 255), thickness=-1) cropped_image = cv2.bitwise_and(cropped_image, mask) cropped_image_resized = cv2.resize(cropped_image, out_size) return cropped_image_resized Here is an example of the complete preprocessing applied to each image:\nNaive Approach #Before implementing a more sophisticated model/algorithm, I want to explore a simple and naive method to see how far it can takes me. This is usualy a rule of thumb for all the projects I\u0026rsquo;ve worked on.\nThe naive method I used is called Thresholding4. It is commonly used in digital image processing.\nDespite its simplicity, this method can be very effective for certain types of images without needing any advanced machine learning, in example like images captured in other spectrums (e.g. IR, UV) or images captured using a fluorescence cameras (\\$\\$\\$) which is very effective for highlighting organic materials from its surrounding.\nExample of Fluorescent Imaging of cancer cells5\rHowever, I will be working with regular RGB colored images of coral plugs, also using images captured in other light spectrum may not be as effective on coral plugs. Since there are other creatures like algaes, baby sea anenemon also growing on the coral plug and those may easily being falsly identified.\nHere is the naive approach:\nFind color pixel value range of the coral babies using k-mean on the pixel values to identify the dominant color palettes in RGB of the corals this process is also known as Quantization6. Before Quantization\rAfter Quantization\rHere I found 10 dominant colors for the coral babies.\nFine tuning Thresholding on each of the dominant color clusters range identified in k-mean. The image below shows how the selected area widens as the value range for each color cluster increases. Blindly expanding these ranges can falsly highlight areas that contains no coral babies:\nThis concludes the naive approach, it requires lots of fine tuning and very difficult to generalized especially when dealing with real-world data.\nActually\u0026hellip;Isn\u0026rsquo;t green screen exactly this? Instead of creating a mask, we just replace the green pixels with pixels from another images? Click me to see the naive approach in Python:\rimport cv2 import numpy as np import glob import matplotlib.pyplot as plt def imshow_pair(img1, img2): \u0026#34;\u0026#34;\u0026#34; Display pair of images with same height side by side Args: img1 (numpy.ndarray): left image img2 (numpy.ndarray): right image returns \u0026#34;\u0026#34;\u0026#34; fig, ax = plt.subplots(1, 2) ax[0].imshow(img1) ax[0].axis(\u0026#34;off\u0026#34;) ax[1].imshow(img2) ax[1].axis(\u0026#34;off\u0026#34;) def concat_vertical(img_list, interpolation=cv2.INTER_CUBIC): \u0026#34;\u0026#34;\u0026#34; Concat images of different sizes vertically. Images are resize such that it will match the mininum image width in the image list Args: img_list (list): List of image to concat. interpolation (int): OpenCV interpolation method for resizing Returns: int: The product of a and b. \u0026#34;\u0026#34;\u0026#34; min_width = min(img.shape[1] for img in img_list) img_list_resize = [ cv2.resize( img, (min_width, int(img.shape[0] * min_width / img.shape[1])), interpolation=interpolation, ) for img in img_list ] return cv2.vconcat(img_list_resize) def find_palette(img, n_colors=10): \u0026#34;\u0026#34;\u0026#34; Returns the n colors palettes from image Args: img (numpy.ndarray): Input image. n_colors (int): Number of color palettes to return. Returns: int: The product of a and b. \u0026#34;\u0026#34;\u0026#34; pixels = img.reshape(-1, 3) criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, 0.1) ret, labels, palette = cv2.kmeans( np.float32(pixels), n_colors, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS ) _, counts = np.unique(labels, return_counts=True) palette = palette[np.argsort(counts)[::-1]] return labels, palette def create_mask(img, palette, pixel_range_window=5): \u0026#34;\u0026#34;\u0026#34; Create mask from image based on palette information Args: img (numpy.ndarray): Input image. palette (numpy.ndarray): Color palette values. pixel_range_window (int): width of the color range for each palette value Returns: coral_withMask (numpy.ndarray): The original coral image \u0026#34;\u0026#34;\u0026#34; new_img_blur = cv2.GaussianBlur(img, (15, 15), 0) new_img_flat = cv2.cvtColor(new_img_blur, cv2.COLOR_BGR2HSV).reshape((-1, 3)) mask = np.uint8(np.zeros(new_img_blur.shape[:2])) for i in range(len(palette)): lower = palette[i] - pixel_range_window upper = palette[i] + pixel_range_window mask = cv2.bitwise_or(mask, cv2.inRange(new_img_blur, lower, upper)) no_coral = cv2.bitwise_and(img, img, mask=cv2.bitwise_not(mask)) coral = cv2.bitwise_and(img, img, mask=mask) coral_withMask = cv2.addWeighted(no_coral, 0.3, coral, 1.5, 0.0) return coral_withMask, mask # import image references of manually selected coral regions references = glob.glob(r\u0026#34;coral_references\\*.jpg\u0026#34;) # stitch all references into one for analysis stitched_references = cv2.resize( cv2.imread(references[0]), (256, 256), interpolation=cv2.INTER_CUBIC ) for i in references[1:]: stitched_references = concat_vertical( [ stitched_references, cv2.resize(cv2.imread(i), (256, 256), interpolation=cv2.INTER_CUBIC), ] ) # Using k-means to find palettes: labels, palette = find_palette(stitched_references, n_colors=10) # Create mask on image: img_path = r\u0026#34;img.jpg\u0026#34; img = cv2.imread(img_path) coral_withMask, coral_mask = create_mask(img, palette, pixel_range_window=12) imshow_pair( cv2.cvtColor(coral_withMask, cv2.COLOR_BGR2RGB), cv2.cvtColor(coral_mask, cv2.COLOR_BGR2RGB), ) Convolution Neural Network #Convolution Neural Network is popular in solving computer vision tasks, I chose the U-Net7 architecture from the Convolution Neural Networks2 architures family. It is well known for its proven sucess in solving image segmentation problems and its ablility to trained efficiently to produce remarkable results for image segmentation.\nSince I didn\u0026rsquo;t have a existing mask for each coral plug image, I manually created them for each image. These masks highlight the areas where the coral babies are located on the coral plug. The objective of the Neural Network is to use these image-mask pairs to train and accurately generate masks for new images of coral plugs.\nThe U-Net training process follows the standard Neural Network training procedures: spliting images into training, validation and test sets to assess model\u0026rsquo;s performance, defining loss function, optimizer and approporiate evaluation metrics to monitor model outputs during training, applied early stopping to control overfitting, and evaluating the best model on unseen images.\nTransfer Learning #I\u0026rsquo;ve opted to use the U-Net architecture with a ResNet8 encoder backbone and begin with pre-trained weights from the ImageNet9 project. This approach allows me to leverage transfer learning from a more sophisticated model that has been exposed to extensive image data, rather than developing an entirely new architecture from scratch. Additionally, given that I only have an Nvidia RTX 2060 with 6GB GPU, training a larger model from the ground up isn\u0026rsquo;t feasible.\nModel Configs # Image size: 256x256 Batch size: 32 Epoch10: 500 (With Early Stopping11)) Loss Criteria12: Jaccard Loss13 ~ IoU14 Activation Function15: Sigmoid Optimizer16: Adam17 Learning Rate18: 0.001 Results #The training process early stopped after only 83 epochs to avoid overfitting.\nIn terms of metrics, the model demostrated great success in learning and generating accurate masks for the training data. However, its performance on unseen images (validation) wasn\u0026rsquo;t optimal, although still quite satisfactory.\nKey takeaways: the best-performing model achieved a 69.9% Intersection over Union (IoU)14 score and an 78.9% recall rate.\nUpon analyzing the performance metrics, there\u0026rsquo;s a noticeable divergence between the validation images (unseen during training) and the training images. This indicates that the model is susceptible to overfitting, especially given the limited amount of images available which results in a small validation set - typically associated with higher variance. Also, the validation set may include specific cases that were not represented in the training data.\nSome prediction results on unseen images:\nPredict on test image - IoU (71.8%); Left to right: Coral Plug, True Mask, Predicted Mask\rPredict on image without preprocessing; Coral Plug vs Predicted Mask\rPredict on Lenna\rWhat I\u0026rsquo;ve learned #I\u0026rsquo;ve gained significant insights and hands-on experience developing a Neural Network for image segmentation on real-world images. There are many other things to explore for improving the model such as fine tuning trainin parameters, trying different CNN architecture, finding larger datasets and refining the true masks so the model will learn precisely what a baby coral is.\nReferences #Here is the notebook on the model building process.\nNotebook\r1\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rImage Segmentation 2\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rConvolutional Neural Network 3Hough Circle Transform 4\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rThresholding 5Credit: Nicola Ferrari @iStock 6\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rQuantization 7\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rU-Net 8\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rResnet 9\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rImageNet 10\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rEpoch 11\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rEarly Stopping 12\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rLoss Criteria 13\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rJaccard Loss 14\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rIoU 15\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rActivation Function 16\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rOptimizer 17\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rAdam 18\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rLearning Rate\n","date":"06 Jan, 2023","permalink":"/projects/coral_baby/","section":"Projects","summary":"Process of creating robust image segmentation of coral babies","title":"Image Segmentation of Coral Baby"},{"content":"","date":null,"permalink":"/tags/neural-network/","section":"Tags","summary":"","title":"Neural Network"},{"content":"","date":null,"permalink":"/tags/opencv/","section":"Tags","summary":"","title":"OpenCV"},{"content":"","date":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"PyTorch"},{"content":"","date":null,"permalink":"/tags/transfer-learning/","section":"Tags","summary":"","title":"Transfer Learning"},{"content":"","date":null,"permalink":"/tags/classification/","section":"Tags","summary":"","title":"Classification"},{"content":"","date":null,"permalink":"/tags/feature-engineering/","section":"Tags","summary":"","title":"Feature Engineering"},{"content":"\rPhoto of a Photo ¬© Steven Lio\rIntroduction #The goal of this project was to develope a classification machine learning model as a proof-of-concept capable of reliably detecting wheather an image is an original photograph or a scanned reproduction of another picture for Trusting Pixel Inc. (Vancouver, Canada). This project served as my Capstone project during my Master of Data Science program at the University of British Columbia where I collaborated with three other students.\nDisclaimer: Due to NDA, I am not able to disclose details of the model but I will document the process of discovery and challenges we faced during the project.\nBackground #Image authenticity recognition is crucial for upholding trust and credibility in digital content, preventing misinformation, and protecting intellectual property. Advancements in deep learning models like DALL-E which is capable of generating realistic images which prompt more reseaches focus into image authenitication. But I believe most of these researches maybe hidden from the public because revealing the principle behind any security system will immediately compromise its effectiveness.\nFor most people, likely the \u0026ldquo;photo of a photo\u0026rdquo; problem isn\u0026rsquo;t significant. However, for companies in the fashion and entertainment industries, they rely heavily on reviewing unaltered professional headshots or body shots of the models, actors, and actresses they are considering for hire.\nThis project piqued my interest because I have been experimenting with different computer vision techniques even before getting my master\u0026rsquo;s degree and have always wanted to gain more experiences in developing Neural Networks.\nData Preprocessing #Trusting Pixel Inc. provided the project with a collection of geninue digital images as well as their recaptured version on the printed photo. The first challenge we faced was the lack of linkage between the geninue images and their recaptured counterparts. This is important for developing a model that extract clues and evidence unique to recaptured images of photos and not memorizing the subjects in the training images.\nI developed a process in Python for automatically pairing the geninue image with its recaptured counterpart and return the pair in the same orientation.\nIn order to find the similar pair of images, I\u0026rsquo;ve first calculated a similarity score (0 to 1) between the image and all the other images in the set. Then the image in the set with the highest score will get paired up. Then repeat until all images has found at least a pair. I\u0026rsquo;ve calculated a similarity score for each possible pair of images using the SIFT1 algorithm.\nIt worked amazingly well even when I am trying to pair a geninue images with a picture of its printed photo.\nClick here to see a implementation of SIFT Similarity calculation in Python:\rimport cv2 import matplotlib.pyplot as plt def sift_similarity(img1, img2): \u0026#34;\u0026#34;\u0026#34; Calculate similarity scores based on SIFT descriptors Args: img1 (numpy.ndarray): Input image 1 img2 (numpy.ndarray): Input image 2 Returns: float: similarity scores between image 1 and image 2 \u0026#34;\u0026#34;\u0026#34; sift = cv2.SIFT_create() kp_1, desc_1 = sift.detectAndCompute(img1, None) kp_2, desc_2 = sift.detectAndCompute(img2, None) index_params = dict(algorithm=0, trees=5) search_params = dict() flann = cv2.FlannBasedMatcher(index_params, search_params) matches = flann.knnMatch(desc_1, desc_2, k=2) good_points = [] ratio = 0.6 for m, n in matches: if m.distance \u0026lt; ratio*n.distance: good_points.append(m) #calculate similarity score: number_keypoints = 0 if len(kp_1) \u0026lt;= len(kp_2): number_keypoints = len(kp_1) else: number_keypoints = len(kp_2) try: match_rate = len(good_points) / number_keypoints except: match_rate = 0 return match_rate # Test on images img1 = cv2.imread(\u0026#34;img1.jpeg\u0026#34;) img2 = cv2.imread(\u0026#34;img2.jpeg\u0026#34;) img3 = cv2.imread(\u0026#34;img3.jpeg\u0026#34;) # Display images fig, axes = plt.subplots(1, 3, figsize=(15,5)) axes[0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)) axes[0].set_title(\u0026#39;Image 1\u0026#39;) axes[0].axis(\u0026#39;off\u0026#39;) axes[1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)) axes[1].set_title(\u0026#39;Image 2\u0026#39;) axes[1].axis(\u0026#39;off\u0026#39;) axes[2].imshow(cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)) axes[2].set_title(\u0026#39;Image 3\u0026#39;) axes[2].axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() # Print scores to console print(\u0026#34;SIFT Similarity between \u0026#39;Image 1\u0026#39; and \u0026#39;Image 1\u0026#39; is {0:.2f}.\u0026#34;.format(sift_similarity(img1, img1))) print(\u0026#34;SIFT Similarity between \u0026#39;Image 1\u0026#39; and \u0026#39;Image 2\u0026#39; is {0:.2f}.\u0026#34;.format(sift_similarity(img1, img2))) print(\u0026#34;SIFT Similarity between \u0026#39;Image 2\u0026#39; and \u0026#39;Image 3\u0026#39; is {0:.2f}.\u0026#34;.format(sift_similarity(img2, img3))) Image 1\rImage 2\rImage 3\rThe output from the Python script:\nSIFT Similarity between \u0026#39;Image 1\u0026#39; and \u0026#39;Image 1\u0026#39; is 1.00. SIFT Similarity between \u0026#39;Image 1\u0026#39; and \u0026#39;Image 2\u0026#39; is 0.67. SIFT Similarity between \u0026#39;Image 2\u0026#39; and \u0026#39;Image 3\u0026#39; is 0.26. After having pairing the images, they were moved to a new folder and renamed to keep track of their pairing, making them ready for the next step.\nModeling #Although Convolution Neural Networks (CNNs) are popular for solving computer vision tasks with images data, it wasn\u0026rsquo;t entirely feasible at the time due to our small dataset and limited computing power, making training a large Neural Network unrealistic.\nThat led us to using traditional classification models. However, these models require a feature matrix as input, and feeding the full-sized images would result in an enormous matrix. After analyzing the geninue images and their recaptured counterparts, we noticed that recaptured images exhibit distinct characteristics. Resizing the images for modeling would have caused us to lose these critical details, which is also why we didn\u0026rsquo;t start with CNNs. The unique features of recaptured images arise from the printing process, where some original image details are lost and new features, like the texture of the print, are added.\nTherefore, we began research image processing techniques to extract these features precisely. We compared the resulting feature value distributions produced by each techniques and to decide the techniques to apply and which features to create.\nWe trained multiple clasifical models with the feature matrix, performing hyperparameter tuning and evaluatng the goodness of the fit to arrive at the final model that best distinguished between geninue image vs. recaptured images for the images we had.\nAlthough we attempted to use CNNs, the results were not significantly better in classifying the testing images. However this does not rule out the possibility that CNNs could eventually prove to be the best overall approach.\nCaveats #The project aims to showcase the potential of using the described method to detect photo of photos as a proof-of-concept. To develop a final model that can handle real-world scenarios, much more image data is needed for training. Additionally, the selected features may only be effective for the specific types of prints from the recaptured images in the training data.\nWe did found some features that showned potential for different types of recaptured images upon testing on various sources. However, I won\u0026rsquo;t be disclosing them.\nFinal Product #Besides the proof-of-concept machine learning model, we\u0026rsquo;ve also created a demo application using Dash in Python. It allows a human judge to process new images within the application and assess the results before making a final decision.\nThe application handles uploaded images, apply the necessary feature extraction, inputs them into the best model for prediction, and displays a confidence score in the app.\nWhat I\u0026rsquo;ve learned #Through this project, I gained extensive knowledge about various image processing techniques, and leaned how combining these techniques can help train a functional image classification machine learning model with limited data.\nReferences #1\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rScale-invariant feature transform (SIFT)\n2Plotly Dash\n","date":"30 Jun, 2022","permalink":"/projects/photo_photo/","section":"Projects","summary":"Using machine learning algorithms to detect Photo of a Photo","title":"Photo of Photo"},{"content":"\rLaunch App\rIntroduction #The aim of this project is to provide an educational tool that enhances understanding of Polynomial Regression1, model complexity, and the trade-offs between model fit and over-fitting in data analysis and predictive modeling.\nThe final product is an interactive app built using R Shiny. This application will allow users to explore polynomial regression by adjusting the degree of the polynomial along with other inputs to observe real-time updatesto the regression line, residual plots and key statistical metrics.\nBackground #During my undergraduate studies, I was fascinated by the numerical methods used to find approximate solutions for problems where exact solutions are difficult to obtain. These methods form the backbone of many machine learning algorithms, such as numerical linear algebra2 for solving regression problems with large datasets, optimization algoithm like gradient descent3 for minimizing cost functions, and backpropagation4 for solving complex chained derivatives when training neural networks5. This fascination inpired me to create an interactive tool that demonstrates these principles in action.\nBuilding the App #The application is built using R Shiny, a powerful tool beloved by R enthusiasts and those interested in creating interactive dashboards using the wide range of statistical modeling packages native in R.\nR Shiny simplifies deployment to local environments with minimal effort, although mastering its UI syntax requires some learning, unlike tools such as Dash that supports syntax similar to HTML in Python.\nThe source code can be found here at GitHub.\nHow to use the App #User Inputs:\nInput expression for the function f(x) to generate sample data points for the application Adjust the range of input values for x in Value Range for x input box User can increase or decrease the Number of Points to be generated Set Seed for Noise RNG for generating the same set of random points Adjust the Variance Level for Noise in the data - higher variance makes the data noisier and appear more random Max Polynomial Degree Fitted controls the maximum degree of the polynomial used. Click the tiny Play button to visualize animated changes as higher polynomial degrees are added Plot X Axis Range to change the area of the plot to be viewed based on the ranges of the x values Tabs:\nPlot\nUser inputs and the main plot area for visualization as well as the regression summary details Residual Plots\nResidual plots of the model predictions vs actual values as higher degree polynomials is added Data\nAll raw input and model prediction values in a table Conclusion #Thank you for reading! If you like to learn more details about Polynomial Regression1 you can also check out this article I wrote on Medium Polynomial Regression„Ä∞Ô∏è and play with the app as you read it or leave a comment below (GitHub required).\nReferences #1\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rPolynomial Regression 2\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rNumerical Linear Algebra 3\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rGradient Descent 4\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rBackpropagation 5\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rNeural Networks\n","date":"06 Feb, 2021","permalink":"/projects/polyfit/","section":"Projects","summary":"Interactive R Shiny App to Visualize Polynomial Regression","title":"Polyfit„Ä∞Ô∏è"},{"content":"","date":null,"permalink":"/tags/predictive-modeling/","section":"Tags","summary":"","title":"Predictive Modeling"},{"content":"","date":null,"permalink":"/tags/r-shiny/","section":"Tags","summary":"","title":"R Shiny"},{"content":"\rA picture of me - Jun, 2024\rHi! ‰Ω†Â•Ω! üëã\nMy name is Steven Lio. I was born in Hong Kong, China and grew up in Macau, China. I moved to Canada in 2007 and currently living in Toronto, Ontario.\nI received my Master of Data Science from University of British Columbia (Canada) and my Bachelor of Science degree in Statistics from McMaster University (Canada).\nWith over 6 years of professional experience, I specialized in developing data-driven solutions and workflows to extract actionable insights that support strategic decision-making for cross-functional teams in Government, Telecom, Retail and Software industries. I possess a natural curiousity and a keen eye for detail when analyzing data, enabling me to find the optimal solutions for finding data-driven solutions and drive business decisions.\nProficient in Python and R, I excel in creating end-to-end machine learning workflows for data science projects. My expertise includes developing efficient data pipelines, conducting feature engineering, implementing machine learning models and building interactive visualization applications. Like this one here.\nI have practical experience tackling real-world challenges by applying machine learning algorithms for a variety of tasks. These include classification, clustering, statistical inference, regression modeling, time series modeling, all aimed at enhancing analytical insights and decision-making process. I have also developed advanced machine learning models such as implementing and training neural networks (CNN) for image classification, segmentation, computer vision tasks. My expertise extends to writing complex SQL queries for processing and aggregating large datasets, and designing robust data ETL processes.\nMy advanced data analytics have supported marketing strategy research in areas such as Predicting Customer Value, Customer Segmentation, Sentiment Analysis, Market Potential and Shares Estimation, Competitor Analysis, Optimizing Sales Channels.\nSomething else about me #On my free time, I enjoyed roaming in the nature and photography. I loves animals and I have a cat named \u0026ldquo;H«îH«î\u0026rdquo; (Tiger in Mandarin Chinese). I also play the piano and guitar. My favourites composers are Chopin and Beethoven.\nIf you are interested to learn more about me, feel free to message me on LinkedIn, or follow me on Instagram.\nFavourite quotes: #\u0026ldquo;MEOW~\u0026rdquo; - HuHu\n\u0026ldquo;All models are wrong, but some are useful\u0026rdquo; - George Box\n\u0026ldquo;We are the representatives of the cosmos; we are an example of what hydrogen atoms can do, given 15 billion years of cosmic evolution.\u0026rdquo; - Carl Sagan\nCheck out the projects I\u0026rsquo;ve previously worked on:\nLearn More\r","date":null,"permalink":"/about/","section":"","summary":"A picture of me - Jun, 2024\rHi!","title":"About me"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"Technical Skills # Programming Languages: Python\r, R\r, SQL\rC#\r, VB.NET\r, .Net Development\rHtml\r, CSS\rMachine Learning Libraries: PyTorch\r, TensorFlow\r, scikit-learn\r, SciPy\r, pandas\r, numpy\r, OpenCV\r, Pillow\rData Visualization: Dash\r, Shiny\r, Plotly\r, Vega-Altair\r, matplotlib\r, ggplot\rTableau\r, PowerBI\rSoftware Development: Git\r, Github\r, Jira\r, Jenkins\r, Docker\r, PowerShell\r, bash\rOther tools: Microsoft SQL Server\r, Visual Studio\rArcGIS\r, QGIS\rSAS EG\r, MATLAB\r, MicroStrategy\rPower Automate Desktop\r, UiPath\r, Automation 360\r, BluePrism\rAnalytics topics: Customer Value Prediction\r, Customer Segmentation\r, Sentiment Analysis\rSeaonality Analysis\r, Market Potential and Shares Estimation\r, Competitor Analysis\rPre \u0026 Post Campaign Analysis\r, Optimizing Sales Channel\rProfessional Experience #\rData Scientist ¬∑ Blueprint Software System\nSep, 2022 ‚Äì Jun, 2024\nQuality Assurance Business Analyst ¬∑ Blueprint Software System\nApr, 2021 ‚Äì Aug, 2022\nData Scientist ¬∑ Bell Mobility\nJan, 2020 ‚Äì Dec, 2020\nMarket Insight Analyst ¬∑ Bell Mobility\nAug, 2017 ‚Äì Dec, 2019\nData QA Analyst ¬∑ Environment and Climate Change Canada\nMay, 2016 ‚Äì Apr, 2017\nCo-op / Internship #\rData QA Support ¬∑ Environment and Climate Change Canada\nMay, 2015 ‚Äì Dec, 2015\nAssistant Methodologist ¬∑ Statistics Canada\nJan, 2014 ‚Äì Aug, 2014\nProjects #Image Segmentation of Coral Baby ¬∑ Personal Project\nJan, 2023 - Jun, 2023\nDeveloped an image segmentation model using a U-Net architecture with ResNet backbone, transfer learning using pre-trained weights from ImageNet to highlight area of coral babies in images of coral frags Photo of Photo ¬∑ Trusting Pixels Inc.\nMay, 2022 - Jun, 2022\nCollaborated on a team to develop a proof-of-concept machine learning model to detect recaptured images Built an interactive Dash application incorporating the best model, allowing users to annotate and identify false recaptured images Project Page Polyfit ¬∑ Personal Project\nFeb, 2021 - Feb, 2021\nInteractive R Shiny tool for visualizing curve fitting using polynomials Project Page, GitHub Launch App\rEducation #\rMasters of Data Science\nUniversity of British Columbia\nSep, 2021 - Aug, 2022\nHonors Bachelor of Science - Mathematics and Statistics Co-op\nMcMaster University\nSep, 2011 - Aug, 2016\nMisc. Information # Languages: English, Cantonese, Mandarin Download R√©sum√©\r","date":null,"permalink":"/resume/","section":"","summary":"Technical Skills # Programming Languages: Python\r, R\r, SQL\rC#\r, VB.","title":"R√©sum√©"}]
[{"content":"\rHi there! ‰Ω†Â•Ω! üëã\nWelcome to my personal website!\nI am a dedicated Data Scientist with extensive experience in developing data-driven solutions to drive business performance.\nPlease explore my portfolio to discover projects I\u0026rsquo;ve worked on and topics I find interesting in data science.\nI am always open to collaborations and discussions. Please feel free to reach out to me on Linkedin.\r","date":null,"permalink":"/","section":"","summary":"Hi there!","title":""},{"content":"Random things I wrote.\n","date":null,"permalink":"/blog/","section":"Blog","summary":"Random things I wrote.","title":"Blog"},{"content":"","date":null,"permalink":"/tags/data-science/","section":"Tags","summary":"","title":"Data Science"},{"content":"Hello internet, welcome to my personal website üê£! First time doing this, let\u0026rsquo;s see how it goes!\nDisplay some of my favorite Math equations: #Einstein\u0026rsquo;s famous Mass-energy equivalence formula üåå:\n$$ E = mc^2 $$\nFrom wiki: The formula defines the energy E of a particle in its rest frame as the product of mass m with the speed of light squared (c¬≤).\nThe üîî Curve: With the probability density function:\n$$ f(x | Œº, œÉ^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(x - Œº)^2}{2 \\sigma^2} \\right) $$\nWhere Œº is the mean or expectation of the distribution and œÉ is the standard deviation. i.e. variance of œÉ¬≤. It is also known as the Gaussian Distribution or more commonly known as the Normal Distribution. An example graph of the function:\nIllustration of the Probability Density Function of a Standard Normal Distribution\rEmbedding codes #This is the R code used to generate the above graph.\n#Clear console and environment rm(list = ls()) cat(\u0026#34;\\014\u0026#34;) #attach required library library(ggplot2) library(dplyr) #set mu and sigma parameters for the Normal Curve mu\u0026lt;-0 sigma\u0026lt;-1 #generate data x\u0026lt;-seq(-4*sigma,4*sigma,0.0001)-0.5 y\u0026lt;-dnorm(x,mu,sigma) #density function data\u0026lt;-cbind(x=x,y=y) %\u0026gt;% data.frame() xlab\u0026lt;-c(expression(-3*sigma) ,expression(-2*sigma) ,expression(-sigma) ,expression(mu) ,expression(sigma) ,expression(2*sigma) ,expression(3*sigma) ) p_lab\u0026lt;-pnorm(seq(-3*sigma,3*sigma,sigma),mu,sigma) #Plot: ggplot(data,aes(x,y))+ #Plot area settings: theme_classic()+ theme(plot.title=element_text(size=40,face=\u0026#34;bold\u0026#34;,hjust=0.5) ,panel.grid.major.x=element_line(size = (0.2)) ,axis.text.x=element_blank())+ ggtitle(\u0026#34;Standard Normal Distribution\u0026#34;)+ xlab(\u0026#34;x\u0026#34;)+ #text for the sigma labels annotate(\u0026#34;text\u0026#34;, x = seq(-3*sigma,3*sigma,sigma), y = rep(-0.005,7), label = xlab, family = \u0026#34;\u0026#34;, fontface = 3, size=8) + #text for the density values at each n*sigma area annotate(\u0026#34;text\u0026#34;, x = c(seq(-3*sigma,3*sigma,sigma)-0.5*sigma,0.5*sigma+3*sigma), y = round(c(p_lab[1],diff(p_lab),p_lab[1]),1)^2+0.05, label = paste0(round(c(p_lab[1],diff(p_lab),p_lab[1])*100,1),\u0026#34;%\u0026#34;), family = \u0026#34;\u0026#34;, fontface = 8, size=8) + #display parameter values annotate(\u0026#34;text\u0026#34;, x = rep(2.2*sigma,2), y = c(0.16,0.14), label = c(expression(paste(mu,\u0026#34;=\u0026#34;)), expression(paste(sigma,\u0026#34;=\u0026#34;))), family = \u0026#34;\u0026#34;, fontface = 3, size=8) + annotate(\u0026#34;text\u0026#34;, x = rep(2.4*sigma,2), y = c(0.164,0.143), label = c(mu,sigma), family = \u0026#34;\u0026#34;, fontface = 3, size=7) + scale_x_continuous(breaks=seq(-3*sigma,3*sigma,sigma),limits=c(-3.5*sigma,3.5*sigma))+ ylab(\u0026#34;Probability Density\u0026#34;)+ scale_y_continuous(labels=scales::percent_format(accuracy=1))+ geom_line() Let\u0026rsquo;s talk about some Linear Algebra: #To find a plane of best-fit. Given a data vector with n samples and p parameters:\n$$ \\begin{Bmatrix} y_i, x_{i1},\\cdot \\cdot \\cdot ,x_{ip} \\end{Bmatrix}^{n}_{i=1} $$\nWhere the dependent variable y and the p-size vector of regressors x are assumed to be a linear relationship. Where the error variable Œµ was modeled such that it is the minimum and ideally it experiences an unobserved random variable. i.e. \u0026ldquo;noise\u0026rdquo;.\nThen ideally we want to find Œ≤ where the model has the form:\n$$ y_i = \\beta_0 + \\beta_1 x_1 + \\cdot \\cdot \\cdot + \\beta_p x_{ip} + \\varepsilon_i = x^T \\beta + \\varepsilon $$\nOr simply\n$$ y = X \\beta + \\varepsilon $$\nWhere\n$$ y = \\begin{Bmatrix} y_1, \\cdot \\cdot \\cdot y_n \\end{Bmatrix}^T $$\n$$ X = \\begin{pmatrix} x^T_1 \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ x^T_n \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; x_{11} \u0026amp; \\cdot\\cdot\\cdot \u0026amp; x_{1p} \\\\\\ \u0026amp; \u0026amp; \\cdot \u0026amp; \\\\\\ \u0026amp; \u0026amp; \\cdot \u0026amp; \\\\\\ \u0026amp; \u0026amp; \\cdot \u0026amp; \\\\\\ 1 \u0026amp; x_{n1} \u0026amp; \\cdot\\cdot\\cdot \u0026amp; x_{np} \\end{pmatrix} $$\nand\n$$ \\beta = \\begin{pmatrix} \\beta_0 \\\\\\ \\beta_1 \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\beta_p \\end{pmatrix} , \\varepsilon = \\begin{pmatrix} \\varepsilon_1 \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\varepsilon_n \\end{pmatrix} $$\nHence for a line, you are solving for the equation with one parameter $$y = \\beta_0 + \\beta_1 x$$ and for a plane there will be two parameters. $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$$\nLeast-squares estimation (a.k.a. Line of best fit): #Since y and x are assumed to be a linear relationship and we would like to find the \u0026ldquo;best\u0026rdquo; Œ≤ which solve the system of equations and to minimize Œµ. Hence let: $$ \\varepsilon = L(X,y,\\hat{\\beta})=\\left | X\\hat{\\beta}-y \\right |^2 = (X\\hat{\\beta} - y)^T(X\\hat{\\beta}-y) \\ $$ $$ = y^Ty - y^TX\\hat{\\beta} - \\hat{\\beta}^TX^Ty + \\hat{\\beta}^TX^TX\\hat{\\beta} $$ Where L is called the Loss function, essentially the error term is modeled with the input X, y, Œ≤. Since X, y is the original data we want to \u0026ldquo;fit\u0026rdquo;, therefore we can find the \u0026ldquo;best\u0026rdquo; Œ≤ at which L(X, y, Œ≤) is minimized.\nHence the first derivative of L(X, y, Œ≤):\n$$ \\frac{\\partial L(X,y,\\hat{\\beta})}{\\partial \\hat{\\beta}} = -2X^Ty+2X^TX\\hat{\\beta} $$\nSince X, y is fixed and known and we only interested in the \u0026ldquo;best\u0026rdquo; Œ≤ therefore:\n$$ \\hat{\\beta} = (X^TX)^{-1}X^Ty $$\nThis is the case for the Simple Linear Regression.\nThis concludes the Hello World! üåé Thank you for reading!\n","date":"24 Jun, 2024","permalink":"/blog/hello_world/","section":"Blog","summary":"Hello internet, welcome to my personal website üê£!","title":"Hello World! üåé"},{"content":"","date":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning"},{"content":"","date":null,"permalink":"/tags/math/","section":"Tags","summary":"","title":"Math"},{"content":"Interesting projects I\u0026rsquo;ve done in the past.\n","date":null,"permalink":"/projects/","section":"Projects","summary":"Interesting projects I\u0026rsquo;ve done in the past.","title":"Projects"},{"content":"","date":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":"","date":null,"permalink":"/tags/r/","section":"Tags","summary":"","title":"R"},{"content":"I am working on putting my projects here. Stay tuned and visit back later! Here is me working on it\u0026hellip;.\nLorem ipsum #Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nLorem ipsum #Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n","date":"24 Jun, 2024","permalink":"/projects/stay_tuned/","section":"Projects","summary":"I am working on putting my projects here.","title":"Stay tuned"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"\rA picture of me - Jun, 2024\rHi! ‰Ω†Â•Ω! üëã\nMy name is Steven Lio. I was born in Hong Kong, China and grew up in Macau, China. I moved to Canada in 2007 and currently living in Toronto, Ontario.\nI received my Master of Data Science from University of British Columbia (Canada) and my Bachelor of Science degree in Statistics from McMaster University (Canada).\nWith over 6 years of professional experience, I specialized in developing data-driven solutions and workflows to extract actionable insights that support strategic decision-making for cross-functional teams in Government, Telecom, Retail and Software industries. I possess a natural curiousity and a keen eye for detail when analyzing data, enabling me to find the optimal solutions for finding data-driven solutions and drive business decisions.\nProficient in Python and R, I excel in creating end-to-end machine learning workflows for data science projects. My expertise includes developing efficient data pipelines, conducting feature engineering, implementing machine learning models and building interactive visualization applications. Like this one here.\nI have practical experience tackling real-world challenges by applying machine learning algorithms for a variety of tasks. These include classification, clustering, statistical inference, regression modeling, time series modeling, all aimed at enhancing analytical insights and decision-making process. I have also developed advanced machine learning models such as implementing and training neural networks (CNN) for image classification, segmentation, computer vision tasks. My expertise extends to writing complex SQL queries for processing and aggregating large datasets, and designing robust data ETL processes.\nMy advanced data analytics have supported marketing strategy research in areas such as Predicting Customer Value, Customer Segmentation, Sentiment Analysis, Market Potential and Shares Estimation, Competitor Analysis, Optimizing Sales Channels.\nSomething else about me #On my free time, I enjoyed roaming in the nature and photography. I loves animals and I have a cat named \u0026ldquo;H«îH«î\u0026rdquo; (Tiger in Mandarin Chinese). I also play the piano and guitar. My favourites composers are Chopin and Beethoven.\nIf you are interested to learn more about me, feel free to message me on LinkedIn, or follow me on Instagram.\nFavourite quotes: #\u0026ldquo;MEOW~\u0026rdquo; - HuHu\n\u0026ldquo;All models are wrong, but some are useful\u0026rdquo; - George Box\n\u0026ldquo;We are the representatives of the cosmos; we are an example of what hydrogen atoms can do, given 15 billion years of cosmic evolution.\u0026rdquo; - Carl Sagan\n","date":null,"permalink":"/about/","section":"","summary":"A picture of me - Jun, 2024\rHi!","title":"About me"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"Technical Skills # Programming Languages: Python\r, R\r, SQL\rC#\r, VB.NET\r, .Net Development\rMachine Learning Libraries: PyTorch\r, TensorFlow\r, scikit-learn\r, SciPy\r, pandas\r, numpy\r, OpenCV\r, Pillow\rData Visualization: Dash\r, Shiny\r, Plotly\r, Vega-Altair\r, matplotlib\r, ggplot\rTableau\r, PowerBI\rSoftware Development: Git\r, Github\r, Jira\r, Jenkins\r, Docker\r, PowerShell\r, bash\rOther tools: Microsoft SQL Server\r, Visual Studio\rArcGIS\r, QGIS\rSAS EG\r, MATLAB\r, MicroStrategy\rPower Automate Desktop\r, UiPath\r, Automation 360\r, BluePrism\rAnalytics topics: Customer Value Prediction\r, Segmentation\r, Sentiment Analysis\r, Market Potential and Shares Estimation\r, Competitor Analysis\r, Pre \u0026 Post Campaign Analysis\r, Optimizing Sales Channel\rProfessional Experience #\rData Scientist ¬∑ Blueprint Software System\nSep, 2022 ‚Äì Jun, 2024\rQuality Assurance Business Analyst ¬∑ Blueprint Software System\nApr, 2021 ‚Äì Aug, 2022\rData Scientist ¬∑ Bell Mobility\nJan, 2020 ‚Äì Dec, 2020\rMarket Insight Analyst ¬∑ Bell Mobility\nAug, 2017 ‚Äì Dec, 2019\rData QA Analyst ¬∑ Environment and Climate Change Canada\nMay, 2016 ‚Äì Apr, 2017\rCo-op / Internship #\rData QA Support ¬∑ Environment and Climate Change Canada\nMay, 2015 ‚Äì Dec, 2015\rAssistant Methodologist ¬∑ Statistics Canada\nJan, 2014 ‚Äì Aug, 2014\nProjects #Jan, 2023 - Jun, 2023\nSegmentation of Coral baby ¬∑ Personal Project\nDeveloped an image segmentation model using a U-Net architecture with transfer learning from ResNet50 to identify coral babies in raw images of coral frags May, 2022 - Jun, 2022\nPhoto of Photo detection ¬∑ Capston Project - Trusting Pixels Inc.\nCollaborated with a team to develop a proof-of-concept machine learning model for detecting recaptured images Built an interactive Dash application incorporating the best model, allowing users to annotate and identify false recaptured images Feb, 2021 - Feb, 2021\nPolyfit ¬∑ Personal Project\nInteractive tool for visualizing curve fitting using polynomials using R Shiny Live-demo, GitHub Education #\rMasters of Data Science\nUniversity of British Columbia\nSep, 2021 - Aug, 2022\nHonors Bachelor of Science - Mathematics and Statistics Co-op\nMcMaster University\nSep, 2011 - Aug, 2016\nMisc. Information # Languages: English, Cantonese, Mandarin View my detail R√©sum√© or Download\r","date":null,"permalink":"/experiences/","section":"","summary":"Technical Skills # Programming Languages: Python\r, R\r, SQL\rC#\r, VB.","title":"R√©sum√©"}]
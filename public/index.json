[{"content":"\rI specialize in developing tools and implementing advanced Machine Learning solutions to tackle challenges within Data Science and Data-Driven Analytics.\nLearn More\r","date":null,"permalink":"/","section":"","summary":"I specialize in developing tools and implementing advanced Machine Learning solutions to tackle challenges within Data Science and Data-Driven Analytics.","title":""},{"content":"Random things I wrote.\n","date":null,"permalink":"/blog/","section":"Blog","summary":"Random things I wrote.","title":"Blog"},{"content":"","date":null,"permalink":"/tags/data-science/","section":"Tags","summary":"","title":"Data Science"},{"content":"Hello internet, welcome to my personal website üê£! First time doing this, let\u0026rsquo;s see how it goes!\nDisplay some Math: #Einstein\u0026rsquo;s famous Mass-energy equivalence formula üåå:\n$$ E = mc^2 $$\nFrom wiki: The formula defines the energy E of a particle in its rest frame as the product of mass m with the speed of light squared (c¬≤).\nThe üîî Curve: With the probability density function:\n$$ f(x | Œº, œÉ^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(x - Œº)^2}{2 \\sigma^2} \\right) $$\nWhere Œº is the mean or expectation of the distribution and œÉ is the standard deviation. i.e. variance of œÉ¬≤. It is also known as the Gaussian Distribution or more commonly known as the Normal Distribution. An example graph of the function:\nIllustration of the Probability Density Function of a Standard Normal Distribution\rEmbedding code block #This is the R code used to generate the above graph.\n#Clear console and environment rm(list = ls()) cat(\u0026#34;\\014\u0026#34;) #attach required library library(ggplot2) library(dplyr) #set mu and sigma parameters for the Normal Curve mu\u0026lt;-0 sigma\u0026lt;-1 #generate data x\u0026lt;-seq(-4*sigma,4*sigma,0.0001)-0.5 y\u0026lt;-dnorm(x,mu,sigma) #density function data\u0026lt;-cbind(x=x,y=y) %\u0026gt;% data.frame() xlab\u0026lt;-c(expression(-3*sigma) ,expression(-2*sigma) ,expression(-sigma) ,expression(mu) ,expression(sigma) ,expression(2*sigma) ,expression(3*sigma) ) p_lab\u0026lt;-pnorm(seq(-3*sigma,3*sigma,sigma),mu,sigma) #Plot: ggplot(data,aes(x,y))+ #Plot area settings: theme_classic()+ theme(plot.title=element_text(size=40,face=\u0026#34;bold\u0026#34;,hjust=0.5) ,panel.grid.major.x=element_line(size = (0.2)) ,axis.text.x=element_blank())+ ggtitle(\u0026#34;Standard Normal Distribution\u0026#34;)+ xlab(\u0026#34;x\u0026#34;)+ #text for the sigma labels annotate(\u0026#34;text\u0026#34;, x = seq(-3*sigma,3*sigma,sigma), y = rep(-0.005,7), label = xlab, family = \u0026#34;\u0026#34;, fontface = 3, size=8) + #text for the density values at each n*sigma area annotate(\u0026#34;text\u0026#34;, x = c(seq(-3*sigma,3*sigma,sigma)-0.5*sigma,0.5*sigma+3*sigma), y = round(c(p_lab[1],diff(p_lab),p_lab[1]),1)^2+0.05, label = paste0(round(c(p_lab[1],diff(p_lab),p_lab[1])*100,1),\u0026#34;%\u0026#34;), family = \u0026#34;\u0026#34;, fontface = 8, size=8) + #display parameter values annotate(\u0026#34;text\u0026#34;, x = rep(2.2*sigma,2), y = c(0.16,0.14), label = c(expression(paste(mu,\u0026#34;=\u0026#34;)), expression(paste(sigma,\u0026#34;=\u0026#34;))), family = \u0026#34;\u0026#34;, fontface = 3, size=8) + annotate(\u0026#34;text\u0026#34;, x = rep(2.4*sigma,2), y = c(0.164,0.143), label = c(mu,sigma), family = \u0026#34;\u0026#34;, fontface = 3, size=7) + scale_x_continuous(breaks=seq(-3*sigma,3*sigma,sigma),limits=c(-3.5*sigma,3.5*sigma))+ ylab(\u0026#34;Probability Density\u0026#34;)+ scale_y_continuous(labels=scales::percent_format(accuracy=1))+ geom_line() Some Linear Algebra: #To find a plane of best-fit. Given a data vector with n samples and p parameters:\n$$ \\begin{Bmatrix} y_i, x_{i1},\\cdot \\cdot \\cdot ,x_{ip} \\end{Bmatrix}^{n}_{i=1} $$\nWhere the dependent variable y and the p-size vector of regressors x are assumed to be a linear relationship. Where the error variable Œµ was modeled such that it is the minimum and ideally it experiences an unobserved random variable. i.e. \u0026ldquo;noise\u0026rdquo;.\nThen ideally we want to find Œ≤ where the model has the form:\n$$ y_i = \\beta_0 + \\beta_1 x_1 + \\cdot \\cdot \\cdot + \\beta_p x_{ip} + \\varepsilon_i = x^T \\beta + \\varepsilon $$\nOr simply\n$$ y = X \\beta + \\varepsilon $$\nWhere\n$$ y = \\begin{Bmatrix} y_1, \\cdot \\cdot \\cdot y_n \\end{Bmatrix}^T $$\n$$ X = \\begin{pmatrix} x^T_1 \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ x^T_n \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; x_{11} \u0026amp; \\cdot\\cdot\\cdot \u0026amp; x_{1p} \\\\\\ \u0026amp; \u0026amp; \\cdot \u0026amp; \\\\\\ \u0026amp; \u0026amp; \\cdot \u0026amp; \\\\\\ \u0026amp; \u0026amp; \\cdot \u0026amp; \\\\\\ 1 \u0026amp; x_{n1} \u0026amp; \\cdot\\cdot\\cdot \u0026amp; x_{np} \\end{pmatrix} $$\nand\n$$ \\beta = \\begin{pmatrix} \\beta_0 \\\\\\ \\beta_1 \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\beta_p \\end{pmatrix} , \\varepsilon = \\begin{pmatrix} \\varepsilon_1 \\\\\\ \\cdot \\\\\\ \\cdot \\\\\\ \\varepsilon_n \\end{pmatrix} $$\nHence for a line, you are solving for the equation with one parameter $$y = \\beta_0 + \\beta_1 x$$ and for a plane there will be two parameters. $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$$\nLeast-squares estimation: #Since y and x are assumed to be a linear relationship and we would like to find the \u0026ldquo;best\u0026rdquo; Œ≤ which solve the system of equations and to minimize Œµ. Hence let: $$ \\varepsilon = L(X,y,\\hat{\\beta})=\\left | X\\hat{\\beta}-y \\right |^2 = (X\\hat{\\beta} - y)^T(X\\hat{\\beta}-y) \\ $$ $$ = y^Ty - y^TX\\hat{\\beta} - \\hat{\\beta}^TX^Ty + \\hat{\\beta}^TX^TX\\hat{\\beta} $$ Where L is called the Loss function, essentially the error term is modeled with the input X, y, Œ≤. Since X, y is the original data we want to \u0026ldquo;fit\u0026rdquo;, therefore we can find the \u0026ldquo;best\u0026rdquo; Œ≤ at which L(X, y, Œ≤) is minimized.\nHence the first derivative of L(X, y, Œ≤):\n$$ \\frac{\\partial L(X,y,\\hat{\\beta})}{\\partial \\hat{\\beta}} = -2X^Ty+2X^TX\\hat{\\beta} $$\nSince X, y is fixed and known and we only interested in the \u0026ldquo;best\u0026rdquo; Œ≤ therefore:\n$$ \\hat{\\beta} = (X^TX)^{-1}X^Ty $$\nThis is the case for the Simple Linear Regression.\nThis concludes the Hello World! üåé Thank you for reading!\n","date":"24 Jun, 2024","permalink":"/blog/hello_world/","section":"Blog","summary":"Hello internet, welcome to my personal website üê£!","title":"Hello World! üåé"},{"content":"","date":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning"},{"content":"","date":null,"permalink":"/tags/math/","section":"Tags","summary":"","title":"Math"},{"content":"Interesting projects I\u0026rsquo;ve done in the past.\n","date":null,"permalink":"/projects/","section":"Projects","summary":"Interesting projects I\u0026rsquo;ve done in the past.","title":"Projects"},{"content":"","date":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":"","date":null,"permalink":"/tags/r/","section":"Tags","summary":"","title":"R"},{"content":"I am working on putting my projects here. Stay tuned and I hope you will visit back later! Meanwhile, take a look at my R√©sum√© or check out my first blog here.\nHere is me working on it\u0026hellip;.\nLorem ipsum #Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nLorem ipsum #Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n","date":"24 Jun, 2024","permalink":"/projects/stay_tuned/","section":"Projects","summary":"More projects is being curated here . . .","title":"Stay tuned"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/linear-algebra/","section":"Tags","summary":"","title":"Linear Algebra"},{"content":"\rLaunch App\rIntroduction #The aim of this project is to provide an educational tool that enhances understanding of Polynomial Regression1, model complexity, and the trade-offs between model fit and over-fitting in data analysis and predictive modeling.\nThe final product is an interactive app built using R Shiny. This application will allow users to explore polynomial regression by adjusting the degree of the polynomial along with other inputs to observe real-time updatesto the regression line, residual plots and key statistical metrics.\nBackground #During my undergraduate studies, I was fascinated by the numerical methods used to find approximate solutions for problems where exact solutions are difficult to obtain. These methods form the backbone of many machine learning algorithms, such as numerical linear algebra2 for solving regression problems with large datasets, optimization algoithm like gradient descent3 for minimizing cost functions, and backpropagation4 for solving complex chained derivatives when training neural networks5. This fascination inpired me to create an interactive tool that demonstrates these principles in action.\nBuilding the App #The application is built using R Shiny, a powerful tool beloved by R enthusiasts and those interested in creating interactive dashboards using the wide range of statistical modeling packages native in R.\nR Shiny simplifies deployment to local environments with minimal effort, although mastering its UI syntax requires some learning, unlike tools such as Dash that supports syntax similar to HTML in Python.\nThe source code can be found here at GitHub.\nHow to use the App #User Inputs:\nInput expression for the function f(x) to generate sample data points for the application Adjust the range of input values for x in Value Range for x input box User can increase or decrease the Number of Points to be generated Set Seed for Noise RNG for generating the same set of random points Adjust the Variance Level for Noise in the data - higher variance makes the data noisier and appear more random Max Polynomial Degree Fitted controls the maximum degree of the polynomial used. Click the tiny Play button to visualize animated changes as higher polynomial degrees are added Plot X Axis Range to change the area of the plot to be viewed based on the ranges of the x values Tabs:\nPlot\nUser inputs and the main plot area for visualization as well as the regression summary details Residual Plots\nResidual plots of the model predictions vs actual values as higher degree polynomials is added Data\nAll raw input and model prediction values in a table Conclusion #Thank you for reading! If you like to learn more details about Polynomial Regression1 you can also check out this article I wrote on Medium Polynomial Regression„Ä∞Ô∏è and play with the app as you read it or leave a comment below (GitHub required).\nReferences #1\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rPolynomial Regression 2\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rNumerical Linear Algebra 3\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rGradient Descent 4\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rBackpropagation 5\r\u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\r\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\rNeural Networks\n","date":"06 Feb, 2021","permalink":"/projects/polyfit/","section":"Projects","summary":"Interactive R Shiny App to Visualize Polynomial Regression","title":"Polyfit„Ä∞Ô∏è"},{"content":"","date":null,"permalink":"/tags/predictive-modeling/","section":"Tags","summary":"","title":"Predictive Modeling"},{"content":"","date":null,"permalink":"/tags/shiny/","section":"Tags","summary":"","title":"Shiny"},{"content":"\rA picture of me - Jun, 2024\rHi! ‰Ω†Â•Ω! üëã\nMy name is Steven Lio. I was born in Hong Kong, China and grew up in Macau, China. I moved to Canada in 2007 and currently living in Toronto, Ontario.\nI received my Master of Data Science from University of British Columbia (Canada) and my Bachelor of Science degree in Statistics from McMaster University (Canada).\nWith over 6 years of professional experience, I specialized in developing data-driven solutions and workflows to extract actionable insights that support strategic decision-making for cross-functional teams in Government, Telecom, Retail and Software industries. I possess a natural curiousity and a keen eye for detail when analyzing data, enabling me to find the optimal solutions for finding data-driven solutions and drive business decisions.\nProficient in Python and R, I excel in creating end-to-end machine learning workflows for data science projects. My expertise includes developing efficient data pipelines, conducting feature engineering, implementing machine learning models and building interactive visualization applications. Like this one here.\nI have practical experience tackling real-world challenges by applying machine learning algorithms for a variety of tasks. These include classification, clustering, statistical inference, regression modeling, time series modeling, all aimed at enhancing analytical insights and decision-making process. I have also developed advanced machine learning models such as implementing and training neural networks (CNN) for image classification, segmentation, computer vision tasks. My expertise extends to writing complex SQL queries for processing and aggregating large datasets, and designing robust data ETL processes.\nMy advanced data analytics have supported marketing strategy research in areas such as Predicting Customer Value, Customer Segmentation, Sentiment Analysis, Market Potential and Shares Estimation, Competitor Analysis, Optimizing Sales Channels.\nSomething else about me #On my free time, I enjoyed roaming in the nature and photography. I loves animals and I have a cat named \u0026ldquo;H«îH«î\u0026rdquo; (Tiger in Mandarin Chinese). I also play the piano and guitar. My favourites composers are Chopin and Beethoven.\nIf you are interested to learn more about me, feel free to message me on LinkedIn, or follow me on Instagram.\nFavourite quotes: #\u0026ldquo;MEOW~\u0026rdquo; - HuHu\n\u0026ldquo;All models are wrong, but some are useful\u0026rdquo; - George Box\n\u0026ldquo;We are the representatives of the cosmos; we are an example of what hydrogen atoms can do, given 15 billion years of cosmic evolution.\u0026rdquo; - Carl Sagan\n","date":null,"permalink":"/about/","section":"","summary":"A picture of me - Jun, 2024\rHi!","title":"About me"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"Technical Skills # Programming Languages: Python\r, R\r, SQL\rC#\r, VB.NET\r, .Net Development\rHtml\r, CSS\rMachine Learning Libraries: PyTorch\r, TensorFlow\r, scikit-learn\r, SciPy\r, pandas\r, numpy\r, OpenCV\r, Pillow\rData Visualization: Dash\r, Shiny\r, Plotly\r, Vega-Altair\r, matplotlib\r, ggplot\rTableau\r, PowerBI\rSoftware Development: Git\r, Github\r, Jira\r, Jenkins\r, Docker\r, PowerShell\r, bash\rOther tools: Microsoft SQL Server\r, Visual Studio\rArcGIS\r, QGIS\rSAS EG\r, MATLAB\r, MicroStrategy\rPower Automate Desktop\r, UiPath\r, Automation 360\r, BluePrism\rAnalytics topics: Customer Value Prediction\r, Customer Segmentation\r, Sentiment Analysis\rSeaonality Analysis\r, Market Potential and Shares Estimation\r, Competitor Analysis\rPre \u0026 Post Campaign Analysis\r, Optimizing Sales Channel\rProfessional Experience #\rData Scientist ¬∑ Blueprint Software System\nSep, 2022 ‚Äì Jun, 2024\nQuality Assurance Business Analyst ¬∑ Blueprint Software System\nApr, 2021 ‚Äì Aug, 2022\nData Scientist ¬∑ Bell Mobility\nJan, 2020 ‚Äì Dec, 2020\nMarket Insight Analyst ¬∑ Bell Mobility\nAug, 2017 ‚Äì Dec, 2019\nData QA Analyst ¬∑ Environment and Climate Change Canada\nMay, 2016 ‚Äì Apr, 2017\nCo-op / Internship #\rData QA Support ¬∑ Environment and Climate Change Canada\nMay, 2015 ‚Äì Dec, 2015\nAssistant Methodologist ¬∑ Statistics Canada\nJan, 2014 ‚Äì Aug, 2014\nProjects #Segmentation of Coral baby ¬∑ Personal Project\nJan, 2023 - Jun, 2023\nDeveloped an image segmentation model using a U-Net architecture with transfer learning from ResNet50 to identify coral babies in raw images of coral frags Photo of Photo detection ¬∑ Trusting Pixels Inc.\nMay, 2022 - Jun, 2022\nCollaborated on a team to develop a proof-of-concept machine learning model to detect recaptured images Built an interactive Dash application incorporating the best model, allowing users to annotate and identify false recaptured images Polyfit ¬∑ Personal Project\nFeb, 2021 - Feb, 2021\nInteractive R Shiny tool for visualizing curve fitting using polynomials Project Page, GitHub Launch App\rEducation #\rMasters of Data Science\nUniversity of British Columbia\nSep, 2021 - Aug, 2022\nHonors Bachelor of Science - Mathematics and Statistics Co-op\nMcMaster University\nSep, 2011 - Aug, 2016\nMisc. Information # Languages: English, Cantonese, Mandarin Download R√©sum√©\r","date":null,"permalink":"/resume/","section":"","summary":"Technical Skills # Programming Languages: Python\r, R\r, SQL\rC#\r, VB.","title":"R√©sum√©"}]